[
  {
    "objectID": "course-materials/labs/week-8-lab.html",
    "href": "course-materials/labs/week-8-lab.html",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Lead is an important contaminant in urban areas with well-known impacts on human health, but do non-human animals also face risks from lead exposure? Hitt et al. investigated this question by measuring lead levels in soil and the blood of breeding mockingbirds, as well as egg hatching and offspring development (Hitt et al. 2023). We will replicate parts of their analysis here.\n\n\n\nThe data for this study were deposited in the Dryad Data Repository and are available here. Download the full dataset and put them in the appropriate folder of your RStudio project.\nRead the northern mockingbird nestling data (“NOMO_Nest_Data.csv”) and nestling lead data (“NestlingPb.csv”) into data frames called nest_data and nestlingpb_data, respectively. Filter both data frames to the Uptown and Lakeshore neighborhoods.\n\nnest_data &lt;- read_csv(\"data/NOMO_Nest_Data.csv\") %&gt;% \n  filter(hood %in% c(\"ls\", \"up\"))\n\nnestlingpb_data &lt;- read_csv(\"data/NestlingPb.csv\") %&gt;% \n  filter(hood %in% c(\"lakeshore\", \"uptown\"))\n\n\n\n\nDo mockingbirds in a neighborhood with higher lead concentration have less successful nests?\nWe’ll investigate this question using randomization hypothesis testing.\n\n\nnest_data contains columns hood and bin.status, representing the neighborhood and binary status (at least one chick fledged or not) for each monitored nest. Visualize how nest success (i.e., binary status) varied by neighborhood.\n\nggplot(nest_data, aes(hood, fill = factor(bin.status))) +\n  geom_bar() +\n  scale_fill_brewer(\"Nest success\", palette = \"Dark2\")\n\n\n\nH0: Neighborhood has no effect on nest success\nHA: Neighborhood has an effect on nest success\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in proportions\n2) How would you calculate it?\n\nnest_success &lt;- nest_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(prop = sum(bin.status) / n())\n\npoint_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n\npoint_estimate_success\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  nest_success &lt;- nest_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(prop = sum(bin.status) / n())\n  \n  point_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n  \n  point_estimate_success\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, \n                 color = \"cornflowerblue\", \n                 fill = NA) +\n  geom_vline(xintercept = point_estimate_success, \n             color = \"firebrick\")\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_success)) / \n  length(null_dist)\n\n\n\n\nWe’re using a threshold of alpha=0.05 (by convention). p&gt;0.05, so we fail to reject the null.\nDo we accept the null? NO. NO WE DO NOT.\n\n\n\n\nNow it’s your turn. Perform a similar analysis to investigate whether nestling blood Pb levels vary by neighborhood. Use nestlingpb_data for this part, column blood_ug_dl_pbwt.\nFirst, visualize the blood Pb levels by neighborhood. What’s an appropriate type of visualization for this?\n\n\nH0:\nHA:\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\n2) How would you calculate it?\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\n\n\nWhat’s the p-value?\n\n\n\n\n\n\n\n\nRather than using randomization to simulate the null distribution, it’s often easier to approximate it as a normal distribution.\nDoes nestling blood Pb level have a relationship with feather Pb level?\nFirst, visualize the relationship between the two variables.\n\n\nH0:\nHA:\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\n2) How would you calculate it?\n\n\n\nUse the standard error of the regression coefficient to visualize the distribution of the sample statistic under the null hypothesis.\n\n\n\nWhat’s the p-value? Use pnorm() to get the probability of a point estimate at least as extreme as the observed.\n\n\n\n\n\n\nNote\n\n\n\nOur calculate p-value is much lower than the p-value from the summary of our linear model. In class I told you lm() uses a Student’s t-distribution instead of a normal distribution for calculating the p-value. When sample sizes are large, the normal distribution and t-distribution are virtually identical. With only 22 complete data points, our sample size is relatively small. Therefore the t-distribution has thicker tails and yields a larger p-value. Hence the discrepancy.\n\n\n\n\n\n\n\n\nRevisit the visualization of the blood and feather Pb levels. One point seems to be an extreme outlier, and it seems to be exerting a strong influence on our model. Repeat the previous analysis with that point removed, then answer the question below.\nQuestion: Of the two analyses (with and without the outlier), which had a lower p-value? Does that make it a better analysis?\n\n\n\n\nFor the last part of today’s lab we will construct confidence intervals around the point estimate of the blood Pb level coefficient. Here’s the plan:\n\nSimulate a population of nestlings, with the same relationship between blood and feather Pb levels as the observed sample.\nDraw a new sample of nestlings from the simulated population. Create a confidence interval for the point estimate of the blood Pb level coefficient in this sample.\nRepeat the process 100 times.\n\n~95% of our 95% CIs should contain the population parameter.\n\n\n\n\n\n\n\n\nHow many 95% CIs contain the population parameter?\n\n\n\n\n\nRandomization allows us to simulate the null distribution, which we can use to quantify the probability of our result if the null hypothesis is true.\n\nsample() and replicate() are helpful here!\nRandomization doesn’t make assumptions about the normality of the sample statistic, but it does assume the sample is representative of the population.\n\nBy assuming the sample statistic is normally distributed, we can use standard errors to conduct hypothesis testing.\n\nR will calculate standard errors for us for most sample statistics, such as regression coefficients.\n\nWe can also use standard errors to construct confidence intervals.\n\nBy simulating a population, we demonstrated ~95% of 95% CIs will contain the population parameter."
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#background",
    "href": "course-materials/labs/week-8-lab.html#background",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Lead is an important contaminant in urban areas with well-known impacts on human health, but do non-human animals also face risks from lead exposure? Hitt et al. investigated this question by measuring lead levels in soil and the blood of breeding mockingbirds, as well as egg hatching and offspring development (Hitt et al. 2023). We will replicate parts of their analysis here."
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#get-the-data",
    "href": "course-materials/labs/week-8-lab.html#get-the-data",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "The data for this study were deposited in the Dryad Data Repository and are available here. Download the full dataset and put them in the appropriate folder of your RStudio project.\nRead the northern mockingbird nestling data (“NOMO_Nest_Data.csv”) and nestling lead data (“NestlingPb.csv”) into data frames called nest_data and nestlingpb_data, respectively. Filter both data frames to the Uptown and Lakeshore neighborhoods.\n\nnest_data &lt;- read_csv(\"data/NOMO_Nest_Data.csv\") %&gt;% \n  filter(hood %in% c(\"ls\", \"up\"))\n\nnestlingpb_data &lt;- read_csv(\"data/NestlingPb.csv\") %&gt;% \n  filter(hood %in% c(\"lakeshore\", \"uptown\"))"
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#hypothesis-testing-by-randomization",
    "href": "course-materials/labs/week-8-lab.html#hypothesis-testing-by-randomization",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Do mockingbirds in a neighborhood with higher lead concentration have less successful nests?\nWe’ll investigate this question using randomization hypothesis testing.\n\n\nnest_data contains columns hood and bin.status, representing the neighborhood and binary status (at least one chick fledged or not) for each monitored nest. Visualize how nest success (i.e., binary status) varied by neighborhood.\n\nggplot(nest_data, aes(hood, fill = factor(bin.status))) +\n  geom_bar() +\n  scale_fill_brewer(\"Nest success\", palette = \"Dark2\")\n\n\n\nH0: Neighborhood has no effect on nest success\nHA: Neighborhood has an effect on nest success\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in proportions\n2) How would you calculate it?\n\nnest_success &lt;- nest_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(prop = sum(bin.status) / n())\n\npoint_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n\npoint_estimate_success\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  nest_success &lt;- nest_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(prop = sum(bin.status) / n())\n  \n  point_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n  \n  point_estimate_success\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, \n                 color = \"cornflowerblue\", \n                 fill = NA) +\n  geom_vline(xintercept = point_estimate_success, \n             color = \"firebrick\")\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_success)) / \n  length(null_dist)\n\n\n\n\nWe’re using a threshold of alpha=0.05 (by convention). p&gt;0.05, so we fail to reject the null.\nDo we accept the null? NO. NO WE DO NOT.\n\n\n\n\nNow it’s your turn. Perform a similar analysis to investigate whether nestling blood Pb levels vary by neighborhood. Use nestlingpb_data for this part, column blood_ug_dl_pbwt.\nFirst, visualize the blood Pb levels by neighborhood. What’s an appropriate type of visualization for this?\n\n\nH0:\nHA:\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\n2) How would you calculate it?\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\n\n\nWhat’s the p-value?"
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#hypothesis-testing-by-normal-approximation",
    "href": "course-materials/labs/week-8-lab.html#hypothesis-testing-by-normal-approximation",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Rather than using randomization to simulate the null distribution, it’s often easier to approximate it as a normal distribution.\nDoes nestling blood Pb level have a relationship with feather Pb level?\nFirst, visualize the relationship between the two variables.\n\n\nH0:\nHA:\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\n2) How would you calculate it?\n\n\n\nUse the standard error of the regression coefficient to visualize the distribution of the sample statistic under the null hypothesis.\n\n\n\nWhat’s the p-value? Use pnorm() to get the probability of a point estimate at least as extreme as the observed.\n\n\n\n\n\n\nNote\n\n\n\nOur calculate p-value is much lower than the p-value from the summary of our linear model. In class I told you lm() uses a Student’s t-distribution instead of a normal distribution for calculating the p-value. When sample sizes are large, the normal distribution and t-distribution are virtually identical. With only 22 complete data points, our sample size is relatively small. Therefore the t-distribution has thicker tails and yields a larger p-value. Hence the discrepancy.\n\n\n\n\n\n\n\n\nRevisit the visualization of the blood and feather Pb levels. One point seems to be an extreme outlier, and it seems to be exerting a strong influence on our model. Repeat the previous analysis with that point removed, then answer the question below.\nQuestion: Of the two analyses (with and without the outlier), which had a lower p-value? Does that make it a better analysis?"
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#confidence-intervals",
    "href": "course-materials/labs/week-8-lab.html#confidence-intervals",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "For the last part of today’s lab we will construct confidence intervals around the point estimate of the blood Pb level coefficient. Here’s the plan:\n\nSimulate a population of nestlings, with the same relationship between blood and feather Pb levels as the observed sample.\nDraw a new sample of nestlings from the simulated population. Create a confidence interval for the point estimate of the blood Pb level coefficient in this sample.\nRepeat the process 100 times.\n\n~95% of our 95% CIs should contain the population parameter.\n\n\n\n\n\n\n\n\nHow many 95% CIs contain the population parameter?"
  },
  {
    "objectID": "course-materials/labs/week-8-lab.html#recap",
    "href": "course-materials/labs/week-8-lab.html#recap",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Randomization allows us to simulate the null distribution, which we can use to quantify the probability of our result if the null hypothesis is true.\n\nsample() and replicate() are helpful here!\nRandomization doesn’t make assumptions about the normality of the sample statistic, but it does assume the sample is representative of the population.\n\nBy assuming the sample statistic is normally distributed, we can use standard errors to conduct hypothesis testing.\n\nR will calculate standard errors for us for most sample statistics, such as regression coefficients.\n\nWe can also use standard errors to construct confidence intervals.\n\nBy simulating a population, we demonstrated ~95% of 95% CIs will contain the population parameter."
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html",
    "href": "course-materials/labs/week-6-lab.html",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "",
    "text": "Is exposure to environmental hazards influenced by class or race and ethnicity? Switzer and Teodoro (Switzer and Teodoro 2017) argued the either-or framing of that question is misguided, and actually the interaction between class and race/ethnicity is the important question for environmental justice. Today’s lab will use logistic regression and interaction terms to investigate drinking water quality in the context of socio-economic status (SES), race, and ethnicity."
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#water-utility-and-violations-data",
    "href": "course-materials/labs/week-6-lab.html#water-utility-and-violations-data",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Water utility and violations data",
    "text": "Water utility and violations data\n\nSource: Safe Drinking Water Information System (SDWIS)\nTime period: 2010-2013 (4 years)\nSample size: 12,972 utilities\nCriteria: Local government-owned utilities serving populations of 10,000 or more"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#demographic-data",
    "href": "course-materials/labs/week-6-lab.html#demographic-data",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Demographic data",
    "text": "Demographic data\n\nSource: US Census Bureau’s American Community Surveys (2010-2013)\nVariables included:\n\nPercent Hispanic population\nPercent Black population\nPercent of population with high school education\nPercent of population with bachelor’s degree\nPercent of population below poverty line\nMedian household income\n\n\nLoad the data and begin exploring.\n\nsuppressMessages(library(tidyverse))\ntheme_set(theme_bw())\n\ndrinking_water &lt;- read_csv(\"drinking_water.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#questions",
    "href": "course-materials/labs/week-6-lab.html#questions",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nWhat do you think each row represents?\nThe water utility and demographic data for a single district in a single year.\nWhat columns do you think represent:\n\nDrinking water health violations?\nhealth\nPercent Black and Hispanic population in the utility district?\npctblack and pcthisp, respectively\nMedian household income in the utility district?\nmedianincomehouse\n\nOne column includes a count of drinking water health violations. How would you create a new column with a binary variable representing whether there were any violations?\n\n\ndrinking_water$violations &lt;- ifelse(drinking_water$health &gt; 0, 1, 0)\n\n\nCreate a scatter plot of violations against race (percent Black population), ethnicity (percent Hispanic population), and SES (median household income). What visualization issues do we get with a scatter plot? How could you address that?\nPoints are overplotted, we can’t see the overall trend. One option: bin the data and calculate the mean.\n\n\nviolations_by_pctblack &lt;- drinking_water %&gt;% \n  mutate(pctblack = round(pctblack / 5) * 5) %&gt;% \n  group_by(pctblack) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(pctblack, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pctblack, color = \"red\")\n\nviolations_by_pcthisp &lt;- drinking_water %&gt;% \n  mutate(pcthisp = round(pcthisp / 5) * 5) %&gt;% \n  group_by(pcthisp) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(pcthisp, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pcthisp, color = \"red\")\n\nviolations_by_medianincomehouse &lt;- drinking_water %&gt;% \n  mutate(medianincomehouse = round(medianincomehouse / 1e4) * 1e4) %&gt;% \n  group_by(medianincomehouse) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(medianincomehouse, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_medianincomehouse, color = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#questions-1",
    "href": "course-materials/labs/week-6-lab.html#questions-1",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nPlot the residuals. What pattern do you notice?\nAll the residuals fall along two parallel lines - not normal!\n\n\ndrinking_water %&gt;% \n  select(violations, pcthisp) %&gt;% \n  drop_na() %&gt;% \n  mutate(resid = resid(pcthisp_lm)) %&gt;% \n  ggplot(aes(pcthisp, resid)) +\n  geom_point()\n\n\nPlot the raw data and the predicted values for violations. Are there any obvious problems?\nThe predicted values fall between 0 and 1, so they could conceivably be interpreted as probabilities. But the raw data are far from the line.\n\n\nggplot(drinking_water, aes(pcthisp, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pcthisp, color = \"red\") +\n  geom_abline(intercept = coef(pcthisp_lm)[1],\n              slope = coef(pcthisp_lm)[2])"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#negative-log-likelihood",
    "href": "course-materials/labs/week-6-lab.html#negative-log-likelihood",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Negative log likelihood",
    "text": "Negative log likelihood\nEstimating coefficients is an optimization problem: what combination yields the maximum likelihood? There are two things we can do to make our problem more tractable for optimization.\n\nRecall that the likelihood function involves a product. Multiplication is computationally costly (compared to addition) and multiplying small numbers is very error prone. We can avoid this problem by working with logarithms. The log of a product is the sum of the logs: \\(log(a \\times b) = log(a) + log(b)\\). Logarithms are monotonically increasing, which means if \\(a &gt; b\\) then \\(log(a) &gt; log(b)\\). This useful property means we can maximize the sum of the log likelihoods (which is quick and robust to errors) instead of the product of likelihoods (which is slow and error prone).\nOptimization algorithms typically find the minimum value. They’re intended to look for valleys, not peaks. So instead of maximizing the log likelihood, we can minimize the negative log likelihood.\n\nThat seems confusing! Let’s write the code and so we can see what’s happening. Do the following:\n\nWrite a likelihood function for the violations and percent Hispanic model. This should calculate the negative log likelihood of a set of coefficients, conditional on the data.\nCall an optimization function to find the maximum likelihood parameters.\n\nIt will help to keep the model formulation handy:\n\\[\n\\begin{align}\n\\text{violations} &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 \\text{percentHispanic}\n\\end{align}\n\\]\n\n# Inverse logit utility function\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\n  \n# Likelihood of the coefficients, given the data\nlikelihood_fun &lt;- function(coefficients, data) {\n  # Calculate logit(p) based on coefficients and predictor\n\n  # Invert the logit to get p\n\n  # Use the PMF of the Bernoulli to get our log likelihoods\n\n  # Sum the negative log likelihood\n  \n}\n\n# Use an optimization function to get the maximum likelihood coefficients\ndrinking_water_complete &lt;- drop_na(drinking_water, pcthisp, violations)\ncoef_optim &lt;- optim(???, \n                    ???, \n                    data = ???)"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#questions-2",
    "href": "course-materials/labs/week-6-lab.html#questions-2",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nWhat were your maximum likelihood estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)?\n\n\n# What are the maximum likelihood estimates for our coefficients?\n# Hint: explore coef_optim\n\n\nWhat’s the predicted probability of drinking water violations for communities with 0%, 50%, and 100% Hispanic population?\nPlot the predicted probability across the whole range 0-100% Hispanic.\n\n\n# Create and plot predictions\n\n\nHow much does the probability of a drinking water violation change when percent Hispanic population increases from 10 to 20%, 45 to 55%, and 80 to 90%?\n\n\nHow would you interpret the coefficients? What do the slope and intercept mean in this context? Where is the relationship linear, and where is it non-linear?\nCreate a “DEM” of the likelihood landscape for \\(\\beta_0\\) and \\(\\beta_1\\). Choose a range of \\(\\beta_0\\) and \\(\\beta_1\\) values around your best estimates, calculate the likelihood for each combination, and create a figure with \\(\\beta_0\\) on the x-axis, \\(\\beta_1\\) on the y-axis, and the likelihood as the fill. Add a point for \\((\\hat\\beta_0, \\hat\\beta_1)\\).\nBonus problem: add contours!\n\n\nlikelihood_dem &lt;- expand_grid(\n  ???,\n  ???\n) %&gt;% \n  mutate(coefficients = mapply(function(b0, b1) c(beta0 = b0, beta1 = b1),\n                               ???, ???,\n                               SIMPLIFY = FALSE),\n         negloglik = sapply(coefficients, \n                            ???, \n                            data = ???),\n         likelihood = ???)\n\nggplot(likelihood_dem, aes(???, ???, fill = ???)) +\n  geom_raster() +\n  geom_point(x = ???, y = ???, color = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week-6-lab.html#questions-3",
    "href": "course-materials/labs/week-6-lab.html#questions-3",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nHow would you fit a model that includes an interaction term between ethnicity and SES?\n\n\ninteraction_glm &lt;- ???\nsummary(interaction_glm)\n\n\nCreate a figure similar to Fig. 1 in Switzer and Teodoro (2017). Put percent Hispanic population on the x-axis, median household income on the y-axis, and make the fill the probability of a water quality violation.\n\n\npredictions &lt;- expand_grid(\n  pcthisp = ???,\n  medianincomehouse = ???\n) %&gt;% \n  mutate(violations = predict(???, \n                              newdata = ., \n                              type = ???))\n\n# Create plot\n\n\nInterpret the predicted surface. How does SES influence the relationship between ethnicity and exposure to environmental hazards? What is the “slope” of the probability of a violation w.r.t. percent Hispanic population at low, medium, and high median household income levels?"
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides-key.html",
    "href": "course-materials/slides/week7/week7-slides-key.html",
    "title": "EDS222 Week 7",
    "section": "",
    "text": "This week, we will review key topics from earlier weeks and postpone new material to week 8. As you complete the following exercises, please try to identify your “muddiest” areas. Are there formulas or functions you’re using and can’t articulate why? Are there similar topics that you’re having a hard time differentiating? Make a note of these points as you go - that will help guide your studying and give me a better idea of how to clarify concepts and tools in our final weeks."
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides-key.html#probability",
    "href": "course-materials/slides/week7/week7-slides-key.html#probability",
    "title": "EDS222 Week 7",
    "section": "Probability",
    "text": "Probability\n\nWhat potential values can a \\(Normal\\) variable take? What about a \\(Bernoulli\\) variable?\nNormal: real numbers. Bernoulli: 0’s and 1’s.\n\nWhat’s something in the real world we could represent with a \\(Normal\\) variable?\nHow about a \\(Bernoulli\\) variable?\n\nWhat’s the difference between a probability density function (PDF) and a probability mass function (PMF)?\nPDF: continuous, PMF: discrete.\n\nConsider distributions from the \\(Normal\\) and \\(Bernoulli\\) families. Which has a PDF? Which a PMF?\nConsider the PDF for a \\(Normal(\\mu=0,\\sigma=1)\\) variable. What’s the value of the PDF when \\(x=0\\)? Does this value represent the probability of producing a 0 from this distribution? Why or why not?\nValue is whatever dnorm(0, mean = 0, sd = 1) gives you. It’s not the probability because the probability of an exact value for a continuous variable is always 0. Think probability = area under the curve, an exact value has no width so it can have no area.\nConsider the PMF for a \\(Bernoulli(p=0.5)\\) variable. What’s the value of the PMF when \\(x=0\\)? Does this value represent the probability of producing a 0 from this distribution? Why or why not?\nIn a PMF it is a probability because the variable takes on discrete values (no area to worry about).\n\nDraw the PMF for a \\(Bernoulli(p=0.25)\\) variable by hand. Then use ggplot to recreate your drawing.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat’s the probability of observing \\(\\{ 0, 0, 1, 1 \\}\\) from a \\(Bernoulli(p = 0.5)\\) variable?\n\n\np &lt;- 0.5\n(1 - p) * (1 - p) * p * p\n\n[1] 0.0625\n\n\n\nWould that probability go up or down if the variable was a \\(Bernoulli(p = 0.99)\\)?\nDown because with a Bernoulli(p = 0.99) we’d expect more 1’s.\n\n\np &lt;- 0.99\n(1 - p) * (1 - p) * p * p\n\n[1] 9.801e-05\n\n\n\nWhat’s the probability of observing 0 from a \\(Bernoulli(p=0.1)\\) AND 1 from a \\(Bernoulli(p=0.9)\\)?\n\n\n(1 - 0.1) * 0.9\n\n[1] 0.81\n\n\n\nExtending the previous question, what’s the probability of observing \\(\\{0, 0, 0, 1, 0\\}\\) from a \\(Bernoulli\\) variable if \\(p\\) is 0.1 for the first observation and increases by 0.1 for each subsequent observation?\n\n\nobs &lt;- c(0, 0, 0, 1, 0)\np &lt;- c(0.1, 0.2, 0.3, 0.4, 0.5)\nprod(ifelse(obs == 1, p, 1 - p))\n\n[1] 0.1008\n\n\n\nDescribe what each following R function returns.\n\ndnorm()\npnorm()\nrnorm()\ndbinom()\nrbinom()\n\nWhich R function would help you solve problem 4 above? Write the code below.\n\n\nprod(dbinom(c(0, 0, 1, 1), size = 1, prob = 0.5))\n\n[1] 0.0625\n\nprod(dbinom(c(0, 0, 1, 1), size = 1, prob = 0.99))\n\n[1] 9.801e-05"
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides-key.html#link-functions",
    "href": "course-materials/slides/week7/week7-slides-key.html#link-functions",
    "title": "EDS222 Week 7",
    "section": "Link functions",
    "text": "Link functions\n\nFill in the blanks below to specify a logistic regression in statistical notation.\n\\[\n\\begin{align}\ny &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\n\nWhat kind of random variable is the response?\nBernoulli\nWhat parameter does that variable take? p\nWhat transformation (link function) is applied to the parameter? logit\nWhich part of the model is linear? \\(\\beta_0 + \\beta_1 x\\)\n\nWhat’s the formula for odds?\n\\(\\frac{p}{1-p}\\)\nWhat’s the formula for the logit function?\n\\(log(\\frac{p}{1-p})\\)\nThe logistic function is the inverse of the logit function. Without using R or a calculator, what’s the value of \\(logistic(logit(0))\\)? How about \\(logistic(logit(1000))\\)?\n0 and 1000, respectively. logistic and logit cancel each other out (they’re inverses).\n\nWhat kinds of numbers can the logit function operate on? Conversely, what inputs would yield an undefined result? Why?\nProbabilities, 0-1. Anything outside that is undefined. Can’t take a log of a negative number.\nWhat about the logistic? Real numbers, -Inf to Inf. Converts them to 0 to 1.\n\nImagine a “logistic” regression without a link function. I.e., \\(p\\) is linearly related to \\(x\\). Give an example of values for \\(\\beta_0,\\beta_1,x\\) that would break your regression model. What constraint on \\(p\\) did you break?\nLet \\(\\beta_0=0, \\beta_1=1, x = 3\\). Then p is 3, which breaks the constraint that it must be a probability.\nIn your own words, describe why the logit link function is necessary for logistic regression to work.\nKeeps the potential values for p in the valid range.\nUse ggplot to create a logistic curve. Your x-axis should cover the range -5 to 5. Use \\(\\beta_0=0,\\beta_1=1\\) as your coefficients.\nAnswer the following questions first using your intuition, then check your answers in code.\n\n\nbeta0 &lt;- 0\nbeta1 &lt;- 1\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\ntibble(x = seq(-5, 5, length.out = 100),\n       logit_p = beta0 + beta1 * x,\n       p = inv_logit(logit_p)) %&gt;% \n  ggplot(aes(x, p)) +\n  geom_line() +\n  ylim(0,1)\n\n\n\n\n\n\n\n\n1.  How will your curve change if $\\beta_0$ increases to 3?\\\n**Increasing intercept will shift the curve up.**\n2.  How will your curve change if you flip the sign of $\\beta_1$ to -1?\n**Reversing the sign will flip the curve left-to-right.**"
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides-key.html#likelihood",
    "href": "course-materials/slides/week7/week7-slides-key.html#likelihood",
    "title": "EDS222 Week 7",
    "section": "Likelihood",
    "text": "Likelihood\n\nThe PMF of a random discrete variable describes the probability of observing a value given the parameters. What is the likelihood function relative to that?\nPMF is probability of data given the parameter. Likelihood tells you the likelihood of the parameter given the data. In the real world, we can only observe data, not parameters, so we need likelihood for inference.\nIs the likelihood function directly interpretable? Or rather, in what context are likelihoods meaningful?\nLikelihood is not directly interpretable, we compare it across parameters to find whatever is most likely. So only meaningful in a relative sense.\nConsider this logistic regression model from the week 6 lab.\n\\[\n\\begin{align}\n\\text{healthViolation} &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 \\text{pctHisp}\n\\end{align}\n\\]\nLet’s say you observe the following data.\n\n\n\npctHisp\nhealthViolation\n\n\n\n\n0.00\n1\n\n\n0.15\n0\n\n\n0.30\n0\n\n\n0.45\n1\n\n\n0.60\n1\n\n\n0.75\n0\n\n\n0.90\n1\n\n\n\n\nVisualize these data using ggplot.\nLet \\(\\beta_0=0, \\beta_1=1\\).\n\nCalculate \\(logit(p)\\) across the range of \\(\\text{pctHisp}\\) values.\nCalculate \\(p\\).\nAdd \\(p\\) to your ggplot as a curve. Make it red.\n\nLet \\(\\beta_0=-2, \\beta_1=0.5\\).\n\nRepeat the calculations above and add a curve for \\(p\\), but make it blue.\n\nIntuitively, which curve do you think is more likely?\n\n\n\nNow you’re going to write a likelihood function for the above logistic model.\n\nWhat inputs does this function need?\nWhat value will it return?\n\nConsider the following partially written code chunk.\n\ninv_logit &lt;- function(x) {\n  ____\n}\nlikelihood_fun &lt;- function(coefs, data) {\n  logit_p &lt;- ____\n  p &lt;- inv_logit(____)\n  loglik &lt;- ____(____, size = 1, prob = ____, ____ = TRUE)\n  -____(____)\n}\n\n\nWhat is the length of the vector coefs (i.e. the first parameter of the likelihood function)? What should the names of the vector be ?\nIf data is a data frame, what two columns does it need to have?\nWhy is size = 1 important? Refer to the statistical notation above.\nIn questions 3.2 and 3.3, you visualized \\(p\\) for two different sets of coefficients. Use the likelihood function to calculate the negative log likelihood for both sets of coefficients. Interpret those values.\n\nNow you’ll use optimization to estimate the most likely coefficients. Fill in the partially written code chunk below.\n\nmost_likely &lt;- optim(\n  par = c(____ = ____, ____ = ____),\n  fn = ____,\n  data = ____\n)\n\n\nWhat estimates did you get for \\(\\beta_0, \\beta_1\\)?\nAdd the predicted values of \\(p\\) for these coefficients to your plot with the raw data. How does this curve compare to the other two you added earlier?\n\nThe last thing you’ll do is create a map of the negative log likelihood “landscape”. This should be raster plot with candidate \\(\\beta_0\\)’s on the x-axis, \\(\\beta_1\\)’s on the y-axis, and the fill should be the negative log likelihood. Add a point for the most likely coefficients. Choose your range of \\(\\beta\\)’s so the most likely coefficients fall somewhere in the middle.\n\n# This utility function will make it easier to call the likelihood function within mutate()\nlikelihood_fun2 &lt;- function(beta0, beta1, data) {\n  likelihood_fun(c(beta0 = beta0, beta1 = beta1), data = data)\n}\n# Create a grid of candidate coefficients to map out the likelihood landscape\nexpand_grid(\n  ____ = seq(____, ____, length.out = 20),\n  ____ = seq(____, ____, length.out = 20)\n) %&gt;%\n  # Calculate the negative log likelihood for each pair of coefficients using mapply() and likelihood_fun2()\n  mutate(____ = mapply(FUN = ____, \n                       ____, \n                       ____, \n                       MoreArgs = list(data = health_data))) %&gt;% \n  # Create a \"likelihood DEM\"\n  ggplot() +\n  geom_raster(aes(x = ____, y = ____, fill = ____)) +\n  # Most likely coefficients\n  geom_point(x = ____, y = ____, color = \"red\") +\n  # Make the fill of the likelihood raster cells look like terrain\n  scale_fill_stepsn(colors = terrain.colors(6)) +\n  theme_bw()\n\n\nAre your most likely coefficients down in a valley or up on a peak? Why?\nPick three pairs of coefficients that fall in green, yellow/orange, and pink/white parts of the likelihood landscape. Recreate the plots from question 3 (raw data with predicted \\(p\\) curve) using those three sets of coefficients. Qualitatively describe the shapes of the curves, where the coefficients fall in likelihood landscape, and how well the curves match the data."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Statistics for Environmental Data Science",
    "section": "Course Description",
    "text": "Course Description\nStatistics is the science of collecting, manipulating, and analyzing empirical data. In this class, we will learn the statistical fundamentals that will enable us to draw conclusions about the environment and its interaction with social and economic systems. We will cover fundamental statistical concepts and tools, and then apply and expand upon those tools to learn some temporal and spatial statistical methods that are particularly helpful in environmental data science. Welcome!\n\nSome concepts we’ll cover:\n\nSampling and study design, descriptive statistics\nLinear and logistic regression (univariate and multivariate)\nHypothesis testing and inference\nSpatial weighting, spatial clustering\nTime series analysis, forecasting"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Statistics for Environmental Data Science",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nMax Czapanskiy\nEmail: maxczap@ucsb.edu\n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\n\n\nLeonardo Feitosa\nEmail: lmfeitosa@ucsb.edu"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistics for Environmental Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nEDS 222 was originally developed and taught by Tamma Carleton. This new website houses materials which are heavily reused, adapted from, and inspired by Tamma’s original work."
  },
  {
    "objectID": "course-materials/live-coding/week6/week6-live-coding.html",
    "href": "course-materials/live-coding/week6/week6-live-coding.html",
    "title": "Week 6 Lecture: Logistic Regression",
    "section": "",
    "text": "Given the logistic regression model:\n\\[\n\\begin{align}\ny &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\nWhat’s the likelihood of some values of \\(\\beta_0\\) and \\(\\beta_1\\)?\nRecall our likelihood function is:\n\\[\nL(\\beta_0, \\beta_1) = \\prod_i PMF(y_i, p_i)\n\\]\nWhere \\(PMF(y_i, p_i)\\) is the value of the Bernoulli probability mass function with probability \\(p_i\\) at \\(y_i\\). E.g., \\(PMF(1, 0.5)=0.5\\), \\(PMF(0, 0.75)=0.25\\), and \\(PMF(1, 0.1)=0.1\\).\n\n\n\n\n# Simulate some values for x\n\n# Choose our betas\n\n# Calculate p across x \n\n# Simulate y\n# Note: rbinom() with size = 1 is equivalent to a random Bernoulli variable\n\n# Visualize\n\n\n\n\nCalculate the likelihood of:\n\n\\(\\beta_0=-5\\), \\(\\beta_1 = 1\\)\n\\(\\beta_0=-1\\), \\(\\beta_1 = 0\\)\n\\(\\beta_0=-3\\), \\(\\beta_1 = 0.5\\)\n\n\n# Write our likelihood function \n# Note this uses the x and y values we generated earlier!\n# We're looking for the likelihood of the parameters given the data.\n\n# Apply the likelihood function to some candidate coefficients\n\n\n\n\nCalculate the likelihood of 1,000,000 combinations of \\(\\beta_0\\), \\(\\beta_1\\) in the ranges \\(\\beta_0 \\in [-5, 0]\\) and \\(\\beta_1 \\in [-1, 2]\\).\n\n# Create a data frame with combinations of beta0, beta1\n # Apply the likelihood function using mapply()\n\n# Isolate the parameters with the maximum likelihood\n\n# Visualize!\n\nOur estimates for \\(\\beta_0\\) and \\(\\beta_1\\) chosen by maximum likelihood are and , respectively. Compare those to the values we used in our simulation: -3 and 0.75."
  },
  {
    "objectID": "course-materials/live-coding/week6/week6-live-coding.html#estimating-logistic-regression-coefficients",
    "href": "course-materials/live-coding/week6/week6-live-coding.html#estimating-logistic-regression-coefficients",
    "title": "Week 6 Lecture: Logistic Regression",
    "section": "",
    "text": "Given the logistic regression model:\n\\[\n\\begin{align}\ny &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\nWhat’s the likelihood of some values of \\(\\beta_0\\) and \\(\\beta_1\\)?\nRecall our likelihood function is:\n\\[\nL(\\beta_0, \\beta_1) = \\prod_i PMF(y_i, p_i)\n\\]\nWhere \\(PMF(y_i, p_i)\\) is the value of the Bernoulli probability mass function with probability \\(p_i\\) at \\(y_i\\). E.g., \\(PMF(1, 0.5)=0.5\\), \\(PMF(0, 0.75)=0.25\\), and \\(PMF(1, 0.1)=0.1\\).\n\n\n\n\n# Simulate some values for x\n\n# Choose our betas\n\n# Calculate p across x \n\n# Simulate y\n# Note: rbinom() with size = 1 is equivalent to a random Bernoulli variable\n\n# Visualize\n\n\n\n\nCalculate the likelihood of:\n\n\\(\\beta_0=-5\\), \\(\\beta_1 = 1\\)\n\\(\\beta_0=-1\\), \\(\\beta_1 = 0\\)\n\\(\\beta_0=-3\\), \\(\\beta_1 = 0.5\\)\n\n\n# Write our likelihood function \n# Note this uses the x and y values we generated earlier!\n# We're looking for the likelihood of the parameters given the data.\n\n# Apply the likelihood function to some candidate coefficients\n\n\n\n\nCalculate the likelihood of 1,000,000 combinations of \\(\\beta_0\\), \\(\\beta_1\\) in the ranges \\(\\beta_0 \\in [-5, 0]\\) and \\(\\beta_1 \\in [-1, 2]\\).\n\n# Create a data frame with combinations of beta0, beta1\n # Apply the likelihood function using mapply()\n\n# Isolate the parameters with the maximum likelihood\n\n# Visualize!\n\nOur estimates for \\(\\beta_0\\) and \\(\\beta_1\\) chosen by maximum likelihood are and , respectively. Compare those to the values we used in our simulation: -3 and 0.75."
  },
  {
    "objectID": "course-materials/week0.html",
    "href": "course-materials/week0.html",
    "title": "Course introduction",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nCourse introduction\nNone\nNone\nPreface, Chapters 1-2 in IMS  [optional] Gender Shades and video on RCTs"
  },
  {
    "objectID": "course-materials/week0.html#class-materials",
    "href": "course-materials/week0.html#class-materials",
    "title": "Course introduction",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nCourse introduction\nNone\nNone\nPreface, Chapters 1-2 in IMS  [optional] Gender Shades and video on RCTs"
  },
  {
    "objectID": "course-materials/week0.html#recorded-lecture",
    "href": "course-materials/week0.html#recorded-lecture",
    "title": "Course introduction",
    "section": "Recorded lecture",
    "text": "Recorded lecture\nProf Czapanskiy is out this week. Please watch the recorded lecture videos below.\n\nPart 1: Intro and course motivation\n\n\n\nPart 2: Syllabus\n\n\n\nPart 3: Populations and samples"
  },
  {
    "objectID": "course-materials/week0.html#notes",
    "href": "course-materials/week0.html#notes",
    "title": "Course introduction",
    "section": "Notes",
    "text": "Notes\n\nProf Czapanskiy will be away weeks 0 and 1. For week 0, please watch the recorded lecture, complete the reading, and answer the reading quiz on Canvas.\nHomework assignment 1 will be posted Thursday September 26"
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Spatio-temporal regression",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nSpatio-temporal regression  Slides  Slides - annotated\nNo class (Thanksgiving)\nNone"
  },
  {
    "objectID": "course-materials/week9.html#class-materials",
    "href": "course-materials/week9.html#class-materials",
    "title": "Spatio-temporal regression",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nSpatio-temporal regression  Slides  Slides - annotated\nNo class (Thanksgiving)\nNone"
  },
  {
    "objectID": "course-materials/week9.html#notes",
    "href": "course-materials/week9.html#notes",
    "title": "Spatio-temporal regression",
    "section": "Notes",
    "text": "Notes\n\nLecture this week is recorded (see link to Vimeo)."
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Logistic regression review",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nLogistic regression exercises\nLogistic regression in R  Logistic regression in R - key\nReview your notes from week 6"
  },
  {
    "objectID": "course-materials/week7.html#class-materials",
    "href": "course-materials/week7.html#class-materials",
    "title": "Logistic regression review",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nLogistic regression exercises\nLogistic regression in R  Logistic regression in R - key\nReview your notes from week 6"
  },
  {
    "objectID": "course-materials/week7.html#notes",
    "href": "course-materials/week7.html#notes",
    "title": "Logistic regression review",
    "section": "Notes",
    "text": "Notes\n\nHomework assignment 4 posted on Tuesday, November 12th."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Midterm",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nMidterm review\nMidterm exam\nNo reading"
  },
  {
    "objectID": "course-materials/week5.html#class-materials",
    "href": "course-materials/week5.html#class-materials",
    "title": "Midterm",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nMidterm review\nMidterm exam\nNo reading"
  },
  {
    "objectID": "course-materials/week5.html#notes",
    "href": "course-materials/week5.html#notes",
    "title": "Midterm",
    "section": "Notes",
    "text": "Notes\n\nMidterm (Thursday) will be in class and closed book\nPractice midterm available here"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Ordinary least squares, continued",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nMultiple regression\nMultiple regression in R\nChapter 8 in IMS  (Optional) Chapter 10 in IMS"
  },
  {
    "objectID": "course-materials/week3.html#class-materials",
    "href": "course-materials/week3.html#class-materials",
    "title": "Ordinary least squares, continued",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nMultiple regression\nMultiple regression in R\nChapter 8 in IMS  (Optional) Chapter 10 in IMS"
  },
  {
    "objectID": "course-materials/week3.html#notes",
    "href": "course-materials/week3.html#notes",
    "title": "Ordinary least squares, continued",
    "section": "Notes",
    "text": "Notes\n\nHomework assignment 2 is due Friday, October 18th."
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Time series analysis",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nTime series analysis\nFinal project co-working time\nChapters 2 and 3 in Hyndman & Athanasopoulos"
  },
  {
    "objectID": "course-materials/week10.html#class-materials",
    "href": "course-materials/week10.html#class-materials",
    "title": "Time series analysis",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nTime series analysis\nFinal project co-working time\nChapters 2 and 3 in Hyndman & Athanasopoulos"
  },
  {
    "objectID": "course-materials/week10.html#notes",
    "href": "course-materials/week10.html#notes",
    "title": "Time series analysis",
    "section": "Notes",
    "text": "Notes\n\nFinal project presentations will take place next week Tuesday December 10th 0800-1100 in Bren 1424. We will make arrangements for you to sign up for 30 minute slots within that window - you won’t need to be there the entire time."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "EDS 222 Assignments",
    "section": "",
    "text": "Homework assignments will be distributed through GitHub Classroom. The links below will become active on the Date Posted.\n\n\n\n\n\n\n\n\nHomework\nDate Posted\nDate Due\n\n\n\n\n1 (link)\nThursday 9/26\nMonday 10/14 5pm\n\n\n2 (link)\nTuesday 10/8\nFriday 10/18 11:59pm\n\n\n3 (link)\nTuesday 10/22\nMonday 11/11 11:59pm\n\n\n4 (link)\nFriday 11/15\nWednesday 11/27 11:59pm"
  },
  {
    "objectID": "assignments.html#homework-assignments",
    "href": "assignments.html#homework-assignments",
    "title": "EDS 222 Assignments",
    "section": "",
    "text": "Homework assignments will be distributed through GitHub Classroom. The links below will become active on the Date Posted.\n\n\n\n\n\n\n\n\nHomework\nDate Posted\nDate Due\n\n\n\n\n1 (link)\nThursday 9/26\nMonday 10/14 5pm\n\n\n2 (link)\nTuesday 10/8\nFriday 10/18 11:59pm\n\n\n3 (link)\nTuesday 10/22\nMonday 11/11 11:59pm\n\n\n4 (link)\nFriday 11/15\nWednesday 11/27 11:59pm"
  },
  {
    "objectID": "assignments.html#submission-guidelines",
    "href": "assignments.html#submission-guidelines",
    "title": "EDS 222 Assignments",
    "section": "Submission guidelines",
    "text": "Submission guidelines\nPlease make sure your homework repositories on GitHub will render the assignment Quarto document! When you’re done with your assignment, run the following diagnostics.\n\nCreate a new RStudio project from version control with your assignment GitHub repository URL. Make sure you create it in a temporary folder, separate from where you did your homework.\nOpen the assignment Quarto document and render it.\nCheck the rendered version. Did the document render without errors? Are all the answers to your questions there?\n\nThis process uses the same workflow Leo and Max use to grade your assignments. If you hit an error doing this then so will we and we won’t be able to grade your assignment. Each homework is worth 10% of your grade, so make sure we can grade your work!"
  },
  {
    "objectID": "assignments.html#final-project",
    "href": "assignments.html#final-project",
    "title": "EDS 222 Assignments",
    "section": "Final project",
    "text": "Final project\nIn addition to the homework assignments, a final project proposal in the form of one short paragraph describing your project is due November 11th (through Canvas).\nThe final project consists of a blog post and a presentation. Details available here."
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Summarizing data",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nSummarizing data\nSummarizing data in R\nLab setup and Github Classrooms demo\nChapters 4-6 in IMS  Variance (video)"
  },
  {
    "objectID": "course-materials/week1.html#class-materials",
    "href": "course-materials/week1.html#class-materials",
    "title": "Summarizing data",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nSummarizing data\nSummarizing data in R\nLab setup and Github Classrooms demo\nChapters 4-6 in IMS  Variance (video)"
  },
  {
    "objectID": "course-materials/week1.html#notes",
    "href": "course-materials/week1.html#notes",
    "title": "Summarizing data",
    "section": "Notes",
    "text": "Notes\n\nProf Czapanskiy will be away weeks 0 and 1. Your TA, Leonardo, will cover lecture and lab this week.\nHomework assignment 1 is due next Monday, October 7th."
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Ordinary least squares",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nOrdinary least squares\nOrdinary least squares in R\nLaw of Large Numbers, simulation in R\nChapter 7 in IMS  Covariance (video)  Linear regression with one regressor (video)"
  },
  {
    "objectID": "course-materials/week2.html#class-materials",
    "href": "course-materials/week2.html#class-materials",
    "title": "Ordinary least squares",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nOrdinary least squares\nOrdinary least squares in R\nLaw of Large Numbers, simulation in R\nChapter 7 in IMS  Covariance (video)  Linear regression with one regressor (video)"
  },
  {
    "objectID": "course-materials/week2.html#notes",
    "href": "course-materials/week2.html#notes",
    "title": "Ordinary least squares",
    "section": "Notes",
    "text": "Notes\n\nHomework assignment 2 will be posted on Tuesday, October 8th.\nHomework assignment 1 is due next Monday, October 14th."
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Multiple regression and interactions",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nInteractions\nInteractions review Interactions in R\nContinuous variable interactions\nChapter 8 in IMS  (Optional) Chapter 10 in IMS"
  },
  {
    "objectID": "course-materials/week4.html#class-materials",
    "href": "course-materials/week4.html#class-materials",
    "title": "Multiple regression and interactions",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Discussion section\n Reading\n\n\n\n\nInteractions\nInteractions review Interactions in R\nContinuous variable interactions\nChapter 8 in IMS  (Optional) Chapter 10 in IMS"
  },
  {
    "objectID": "course-materials/week4.html#notes",
    "href": "course-materials/week4.html#notes",
    "title": "Multiple regression and interactions",
    "section": "Notes",
    "text": "Notes\n\nHomework assignment 3 will be posted Tuesday, October 22nd."
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Nonlinear models",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nLogistic regression slides  Live coding demo  Logistic regression slides (annotated)\nLogistic regression in R  Logistic regression in R - key\nChapter 9 in IMS"
  },
  {
    "objectID": "course-materials/week6.html#class-materials",
    "href": "course-materials/week6.html#class-materials",
    "title": "Nonlinear models",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nLogistic regression slides  Live coding demo  Logistic regression slides (annotated)\nLogistic regression in R  Logistic regression in R - key\nChapter 9 in IMS"
  },
  {
    "objectID": "course-materials/week6.html#notes",
    "href": "course-materials/week6.html#notes",
    "title": "Nonlinear models",
    "section": "Notes",
    "text": "Notes\n\nHomework assignment 3 due Friday, November 8th next Monday November 11th.\nFinal project proposal is also due Friday, November 8th Monday November 11th. Submission will be through Canvas.\nThe notes from this week’s office hours are here."
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nHypothesis testing slides  Hypothesis testing slides (annotated)\nHypothesis testing in R  Hypothesis testing in R - key\nChapters 11, 13 in IMS"
  },
  {
    "objectID": "course-materials/week8.html#class-materials",
    "href": "course-materials/week8.html#class-materials",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Lecture slides\n Lab activity\n Reading\n\n\n\n\nHypothesis testing slides  Hypothesis testing slides (annotated)\nHypothesis testing in R  Hypothesis testing in R - key\nChapters 11, 13 in IMS"
  },
  {
    "objectID": "course-materials/week8.html#notes",
    "href": "course-materials/week8.html#notes",
    "title": "Hypothesis testing",
    "section": "Notes",
    "text": "Notes\n\nMidterm regrade due Thursday on Canvas"
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "The goal of this project is to explore an environmental data science question for which you do not know the answer. You will construct a research question, collect relevant data, and design a statistical analysis to answer your question. Your analysis must apply at least some of the statistical concepts you have learned in this course, including concepts covered in the second half of the class. You will summarize your findings and communicate clearly how they have or have not helped you answer your question.\nYour results do not need to be conclusive!\nMost data science questions cannot be fully answered with one analysis. If you carefully conduct your analysis, yet cannot conclude anything concrete,that is perfectly fine. There are many reasons you might find yourself in this situation, including, among many others:you have null results (i.e., you fail to reject the null hypothesis); your results are uncertain (e.g., one test suggests one answer while another test suggests another answer); there are issues with the analysis due to limited data availability; there are issues with the analysis because of violations of OLS assumptions, etc. Regardless of the final results, make sure to carefully describe your data and analysis, and to clearly articulate the limitations of your findings."
  },
  {
    "objectID": "final-project.html#project-goal",
    "href": "final-project.html#project-goal",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "The goal of this project is to explore an environmental data science question for which you do not know the answer. You will construct a research question, collect relevant data, and design a statistical analysis to answer your question. Your analysis must apply at least some of the statistical concepts you have learned in this course, including concepts covered in the second half of the class. You will summarize your findings and communicate clearly how they have or have not helped you answer your question.\nYour results do not need to be conclusive!\nMost data science questions cannot be fully answered with one analysis. If you carefully conduct your analysis, yet cannot conclude anything concrete,that is perfectly fine. There are many reasons you might find yourself in this situation, including, among many others:you have null results (i.e., you fail to reject the null hypothesis); your results are uncertain (e.g., one test suggests one answer while another test suggests another answer); there are issues with the analysis due to limited data availability; there are issues with the analysis because of violations of OLS assumptions, etc. Regardless of the final results, make sure to carefully describe your data and analysis, and to clearly articulate the limitations of your findings."
  },
  {
    "objectID": "final-project.html#submission-guidelines",
    "href": "final-project.html#submission-guidelines",
    "title": "Final Project Guidelines",
    "section": "Submission guidelines",
    "text": "Submission guidelines\nThe submission has three parts:\n\nBrief proposal [Due date: 11/11, 11:59pm]. Please write a short paragraph (4-5 sentences) describing the project you propose to pursue. Motivate the question, describe possible data sources, and suggest possible analyses you may conduct to answer your question. Submission will be through Canvas.\nTechnical blog post [Due date: 12/13, 11:59 pm]. This blog post is a write up summarizing in text and with figures and/or tables your question, the data you have collected, your analysis plan, and your results. Your target audience should be other quantitative scientists and practitioners familiar with the basics of statistics and data science, but not necessarily experts in environmental science or the details of the methods studied in this course. Submit a link to your blog post via Canvas by 11:59pm on December 13.\n\nSome guidelines for the blog post:\n\nMain text length should be roughly 1500 - 2500 words.\n2-4 tables and/or figures, each carefully labeled and captioned so that they are easily interpretable\nInclude scientific references when applicable.\nInclude links to the underlying data you use. If your data cannot be shared publicly, note this in a short “data availability” statement at the end of your post.\nInclude a link to a repository with your code.\n\n\n4-minute presentation [December 10 8-11am]. This is a short presentation for the class in which you motivate your question, describe your data and analysis plan, and summarize your results. It should be a fun way to practice sharing data science with an audience through public speaking.\nIn order to fit everyone in, the 4-minute limit will have to be strictly enforced. Practice your presentation in advance and time yourself! 4 minutes is probably much, much shorter than you think."
  },
  {
    "objectID": "final-project.html#general-guidelines",
    "href": "final-project.html#general-guidelines",
    "title": "Final Project Guidelines",
    "section": "General guidelines",
    "text": "General guidelines\n\nMotivate your question. Why is this important? Is there existing evidence on this question? If so, why is it inconclusive? If not, why not?\nDescribe your data. Where did you access it? What are its spatial and temporal features? What are its limitations? What do you know about the sampling strategy and what biases that may introduce? If helpful, you can use a histogram, scatterplot, or summary statistics table to describe your data.\nClearly describe your analysis plan. What is your analysis plan? Why did you choose this analysis, given your data and question? What are the limitations?\nSummarize your results visually and in words. Show us your results in figure(s) and/or table(s) that are carefully labeled and captioned. Describe in the text (and orally when presenting) what you found, and how these results either do or do not help you answeryour question.\nWhat might you do next? One analysis cannot fully answer an interesting scientific question. If you had time to collect more data or conduct more analyses, what would help you answer this question better?"
  },
  {
    "objectID": "course-materials/live-coding/week6/week6-live-coding-instructor-notes.html",
    "href": "course-materials/live-coding/week6/week6-live-coding-instructor-notes.html",
    "title": "Week 6 Lecture: Logistic Regression",
    "section": "",
    "text": "Given the logistic regression model:\n\\[\n\\begin{align}\ny &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\nWhat’s the likelihood of some values of \\(\\beta_0\\) and \\(\\beta_1\\)?\nRecall our likelihood function is:\n\\[\nL(\\beta_0, \\beta_1) = \\prod_i PMF(y_i, p_i)\n\\]\nWhere \\(PMF(y_i, p_i)\\) is the value of the Bernoulli probability mass function with probability \\(p_i\\) at \\(y_i\\). E.g., \\(PMF(1, 0.5)=0.5\\), \\(PMF(0, 0.75)=0.25\\), and \\(PMF(1, 0.1)=0.1\\).\n\n\n\n\n# Simulate some values for x\nx &lt;- runif(10, 0, 10)\n\n# Choose our betas\nbeta0 &lt;- -3\nbeta1 &lt;- 0.75\n\n# Calculate p across x \nlogit_p &lt;- beta0 + beta1 * x\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\np &lt;- inv_logit(logit_p)\n\n# Simulate y\n# Note: rbinom() with size = 1 is equivalent to a random Bernoulli variable\ny &lt;- rbinom(10, size = 1, p)\n\n# Visualize\ntibble(x, y) %&gt;% \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, color = \"cornflowerblue\", shape = 18) +\n  scale_x_continuous(breaks = seq(0, 10, by = 2), limits = c(0, 10))\n\n\n\n\n\n\n\n\n\n\n\nCalculate the likelihood of:\n\n\\(\\beta_0=-5\\), \\(\\beta_1 = 1\\)\n\\(\\beta_0=-1\\), \\(\\beta_1 = 0\\)\n\\(\\beta_0=-3\\), \\(\\beta_1 = 0.5\\)\n\n\n# Write our likelihood function \n# Note this uses the x and y values we generated earlier!\n# We're looking for the likelihood of the parameters given the data.\nlikelihood_fun &lt;- function(beta0, beta1) {\n  logit_p &lt;- beta0 + beta1 * x\n  p &lt;- inv_logit(logit_p)\n  prod(dbinom(y, 1, p))\n}\n\n# Apply the likelihood function to some candidate coefficients\nlikelihood_fun(-5, 1)\n\n[1] 0.01463619\n\nlikelihood_fun(-1, 0)\n\n[1] 3.976128e-05\n\nlikelihood_fun(-3, 0.5)\n\n[1] 0.005685931\n\n\n\n\n\nCalculate the likelihood of 1,000,000 combinations of \\(\\beta_0\\), \\(\\beta_1\\) in the ranges \\(\\beta_0 \\in [-5, 0]\\) and \\(\\beta_1 \\in [-1, 2]\\).\n\n# Create a data frame with combinations of beta0, beta1\nlikelihood_space &lt;- expand_grid(\n  beta0 = seq(-5, 0, length.out = 1000),\n  beta1 = seq(-1, 2, length.out = 1000)\n) %&gt;% \n  # Apply the likelihood function using mapply()\n  mutate(likelihood = mapply(likelihood_fun, beta0, beta1))\n\n# Isolate the parameters with the maximum likelihood\nmax_likelihood &lt;- filter(likelihood_space, likelihood == max(likelihood))\n\n# Visualize!\nggplot(likelihood_space, aes(beta0, beta1, fill = likelihood)) +\n  geom_raster() +\n  geom_point(data = max_likelihood, color = \"red\") +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nOur estimates for \\(\\beta_0\\) and \\(\\beta_1\\) chosen by maximum likelihood are -3.193 and 0.82, respectively. Compare those to the values we used in our simulation: -3 and 0.75."
  },
  {
    "objectID": "course-materials/live-coding/week6/week6-live-coding-instructor-notes.html#estimating-logistic-regression-coefficients",
    "href": "course-materials/live-coding/week6/week6-live-coding-instructor-notes.html#estimating-logistic-regression-coefficients",
    "title": "Week 6 Lecture: Logistic Regression",
    "section": "",
    "text": "Given the logistic regression model:\n\\[\n\\begin{align}\ny &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\nWhat’s the likelihood of some values of \\(\\beta_0\\) and \\(\\beta_1\\)?\nRecall our likelihood function is:\n\\[\nL(\\beta_0, \\beta_1) = \\prod_i PMF(y_i, p_i)\n\\]\nWhere \\(PMF(y_i, p_i)\\) is the value of the Bernoulli probability mass function with probability \\(p_i\\) at \\(y_i\\). E.g., \\(PMF(1, 0.5)=0.5\\), \\(PMF(0, 0.75)=0.25\\), and \\(PMF(1, 0.1)=0.1\\).\n\n\n\n\n# Simulate some values for x\nx &lt;- runif(10, 0, 10)\n\n# Choose our betas\nbeta0 &lt;- -3\nbeta1 &lt;- 0.75\n\n# Calculate p across x \nlogit_p &lt;- beta0 + beta1 * x\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\np &lt;- inv_logit(logit_p)\n\n# Simulate y\n# Note: rbinom() with size = 1 is equivalent to a random Bernoulli variable\ny &lt;- rbinom(10, size = 1, p)\n\n# Visualize\ntibble(x, y) %&gt;% \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, color = \"cornflowerblue\", shape = 18) +\n  scale_x_continuous(breaks = seq(0, 10, by = 2), limits = c(0, 10))\n\n\n\n\n\n\n\n\n\n\n\nCalculate the likelihood of:\n\n\\(\\beta_0=-5\\), \\(\\beta_1 = 1\\)\n\\(\\beta_0=-1\\), \\(\\beta_1 = 0\\)\n\\(\\beta_0=-3\\), \\(\\beta_1 = 0.5\\)\n\n\n# Write our likelihood function \n# Note this uses the x and y values we generated earlier!\n# We're looking for the likelihood of the parameters given the data.\nlikelihood_fun &lt;- function(beta0, beta1) {\n  logit_p &lt;- beta0 + beta1 * x\n  p &lt;- inv_logit(logit_p)\n  prod(dbinom(y, 1, p))\n}\n\n# Apply the likelihood function to some candidate coefficients\nlikelihood_fun(-5, 1)\n\n[1] 0.01463619\n\nlikelihood_fun(-1, 0)\n\n[1] 3.976128e-05\n\nlikelihood_fun(-3, 0.5)\n\n[1] 0.005685931\n\n\n\n\n\nCalculate the likelihood of 1,000,000 combinations of \\(\\beta_0\\), \\(\\beta_1\\) in the ranges \\(\\beta_0 \\in [-5, 0]\\) and \\(\\beta_1 \\in [-1, 2]\\).\n\n# Create a data frame with combinations of beta0, beta1\nlikelihood_space &lt;- expand_grid(\n  beta0 = seq(-5, 0, length.out = 1000),\n  beta1 = seq(-1, 2, length.out = 1000)\n) %&gt;% \n  # Apply the likelihood function using mapply()\n  mutate(likelihood = mapply(likelihood_fun, beta0, beta1))\n\n# Isolate the parameters with the maximum likelihood\nmax_likelihood &lt;- filter(likelihood_space, likelihood == max(likelihood))\n\n# Visualize!\nggplot(likelihood_space, aes(beta0, beta1, fill = likelihood)) +\n  geom_raster() +\n  geom_point(data = max_likelihood, color = \"red\") +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nOur estimates for \\(\\beta_0\\) and \\(\\beta_1\\) chosen by maximum likelihood are -3.193 and 0.82, respectively. Compare those to the values we used in our simulation: -3 and 0.75."
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides.html",
    "href": "course-materials/slides/week7/week7-slides.html",
    "title": "EDS222 Week 7",
    "section": "",
    "text": "This week, we will review key topics from earlier weeks and postpone new material to week 8. As you complete the following exercises, please try to identify your “muddiest” areas. Are there formulas or functions you’re using and can’t articulate why? Are there similar topics that you’re having a hard time differentiating? Make a note of these points as you go - that will help guide your studying and give me a better idea of how to clarify concepts and tools in our final weeks."
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides.html#probability",
    "href": "course-materials/slides/week7/week7-slides.html#probability",
    "title": "EDS222 Week 7",
    "section": "Probability",
    "text": "Probability\n\nWhat potential values can a \\(Normal\\) variable take? What about a \\(Bernoulli\\) variable?\n\nWhat’s something in the real world we could represent with a \\(Normal\\) variable?\nHow about a \\(Bernoulli\\) variable?\n\nWhat’s the difference between a probability density function (PDF) and a probability mass function (PMF)?\n\nConsider distributions from the \\(Normal\\) and \\(Bernoulli\\) families. Which has a PDF? Which a PMF?\nConsider the PDF for a \\(Normal(\\mu=0,\\sigma=1)\\) variable. What’s the value of the PDF when \\(x=0\\)? Does this value represent the probability of producing a 0 from this distribution? Why or why not?\nConsider the PMF for a \\(Bernoulli(p=0.5)\\) variable. What’s the value of the PMF when \\(x=0\\)? Does this value represent the probability of producing a 0 from this distribution? Why or why not?\n\nDraw the PMF for a \\(Bernoulli(p=0.25)\\) variable by hand. Then use ggplot to recreate your drawing.\n\n\nWhat’s the probability of observing \\(\\{ 0, 0, 1, 1 \\}\\) from a \\(Bernoulli(p = 0.5)\\) variable?\n\nWould that probability go up or down if the variable was a \\(Bernoulli(p = 0.99)\\)?\n\nWhat’s the probability of observing 0 from a \\(Bernoulli(p=0.1)\\) AND 1 from a \\(Bernoulli(p=0.9)\\)?\n\nExtending the previous question, what’s the probability of observing \\(\\{0, 0, 0, 1, 0\\}\\) from a \\(Bernoulli\\) variable if \\(p\\) is 0.1 for the first observation and increases by 0.1 for each subsequent observation?\n\nDescribe what each following R function returns.\n\ndnorm()\npnorm()\nrnorm()\ndbinom()\nrbinom()\n\nWhich R function would help you solve problem 4 above? Write the code below."
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides.html#link-functions",
    "href": "course-materials/slides/week7/week7-slides.html#link-functions",
    "title": "EDS222 Week 7",
    "section": "Link functions",
    "text": "Link functions\n\nFill in the blanks below to specify a logistic regression in statistical notation.\n\\[\n\\begin{align}\ny &\\sim ??? (???) \\\\\n???(???) &= ??? + ??? x\n\\end{align}\n\\]\n\nWhat kind of random variable is the response?\nWhat parameter does that variable take?\nWhat transformation (link function) is applied to the parameter?\nWhich part of the model is linear?\n\nWhat’s the formula for odds?\nWhat’s the formula for the logit function?\nThe logistic function is the inverse of the logit function. Without using R or a calculator, what’s the value of \\(logistic(logit(0))\\)? How about \\(logistic(logit(1000))\\)?\n\nWhat kinds of numbers can the logit function operate on? Conversely, what inputs would yield an undefined result? Why?\nWhat about the logistic?\n\nImagine a “logistic” regression without a link function. I.e., \\(p\\) is linearly related to \\(x\\). Give an example of values for \\(\\beta_0,\\beta_1,x\\) that would break your regression model. What constraint on \\(p\\) did you break?\nIn your own words, describe why the logit link function is necessary for logistic regression to work.\nUse ggplot to create a logistic curve. Your x-axis should cover the range -5 to 5. Use \\(\\beta_0=0,\\beta_1=1\\) as your coefficients.\nAnswer the following questions first using your intuition, then check your answers in code.\n\nHow will your curve change if \\(\\beta_0\\) increases to 3?\nHow will your curve change if you flip the sign of \\(\\beta_1\\) to -1?"
  },
  {
    "objectID": "course-materials/slides/week7/week7-slides.html#likelihood",
    "href": "course-materials/slides/week7/week7-slides.html#likelihood",
    "title": "EDS222 Week 7",
    "section": "Likelihood",
    "text": "Likelihood\n\nThe PMF of a random discrete variable describes the probability of observing a value given the parameters. What is the likelihood function relative to that?\nIs the likelihood function directly interpretable? Or rather, in what context are likelihoods meaningful?\nConsider this logistic regression model from the week 6 lab.\n\\[\n\\begin{align}\n\\text{healthViolation} &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 \\text{pctHisp}\n\\end{align}\n\\]\nLet’s say you observe the following data.\n\n\n\npctHisp\nhealthViolation\n\n\n\n\n0.00\n1\n\n\n0.15\n0\n\n\n0.30\n0\n\n\n0.45\n1\n\n\n0.60\n1\n\n\n0.75\n0\n\n\n0.90\n1\n\n\n\n\nVisualize these data using ggplot.\nLet \\(\\beta_0=0, \\beta_1=1\\).\n\nCalculate \\(logit(p)\\) across the range of \\(\\text{pctHisp}\\) values.\nCalculate \\(p\\).\nAdd \\(p\\) to your ggplot as a curve. Make it red.\n\nLet \\(\\beta_0=-2, \\beta_1=0.5\\).\n\nRepeat the calculations above and add a curve for \\(p\\), but make it blue.\n\nIntuitively, which curve do you think is more likely?\n\nNow you’re going to write a likelihood function for the above logistic model.\n\nWhat inputs does this function need?\nWhat value will it return?\n\nConsider the following partially written code chunk.\n\ninv_logit &lt;- function(x) {\n  ____\n}\nlikelihood_fun &lt;- function(coefs, data) {\n  logit_p &lt;- ____\n  p &lt;- inv_logit(____)\n  loglik &lt;- ____(____, size = 1, prob = ____, ____ = TRUE)\n  -____(____)\n}\n\n\nWhat is the length of the vector coefs (i.e. the first parameter of the likelihood function)? What should the names of the vector be ?\nIf data is a data frame, what two columns does it need to have?\nWhy is size = 1 important? Refer to the statistical notation above.\nIn questions 3.2 and 3.3, you visualized \\(p\\) for two different sets of coefficients. Use the likelihood function to calculate the negative log likelihood for both sets of coefficients. Interpret those values.\n\nNow you’ll use optimization to estimate the most likely coefficients. Fill in the partially written code chunk below.\n\nmost_likely &lt;- optim(\n  par = c(____ = ____, ____ = ____),\n  fn = ____,\n  data = ____\n)\n\n\nWhat estimates did you get for \\(\\beta_0, \\beta_1\\)?\nAdd the predicted values of \\(p\\) for these coefficients to your plot with the raw data. How does this curve compare to the other two you added earlier?\n\nThe last thing you’ll do is create a map of the negative log likelihood “landscape”. This should be raster plot with candidate \\(\\beta_0\\)’s on the x-axis, \\(\\beta_1\\)’s on the y-axis, and the fill should be the negative log likelihood. Add a point for the most likely coefficients. Choose your range of \\(\\beta\\)’s so the most likely coefficients fall somewhere in the middle.\n\n# This utility function will make it easier to call the likelihood function within mutate()\nlikelihood_fun2 &lt;- function(beta0, beta1, data) {\n  likelihood_fun(c(beta0 = beta0, beta1 = beta1), data = data)\n}\n# Create a grid of candidate coefficients to map out the likelihood landscape\nexpand_grid(\n  ____ = seq(____, ____, length.out = 20),\n  ____ = seq(____, ____, length.out = 20)\n) %&gt;%\n  # Calculate the negative log likelihood for each pair of coefficients using mapply() and likelihood_fun2()\n  mutate(____ = mapply(FUN = ____, \n                       ____, \n                       ____, \n                       MoreArgs = list(data = health_data))) %&gt;% \n  # Create a \"likelihood DEM\"\n  ggplot() +\n  geom_raster(aes(x = ____, y = ____, fill = ____)) +\n  # Most likely coefficients\n  geom_point(x = ____, y = ____, color = \"red\") +\n  # Make the fill of the likelihood raster cells look like terrain\n  scale_fill_stepsn(colors = terrain.colors(6)) +\n  theme_bw()\n\n\nAre your most likely coefficients down in a valley or up on a peak? Why?\nPick three pairs of coefficients that fall in green, yellow/orange, and pink/white parts of the likelihood landscape. Recreate the plots from question 3 (raw data with predicted \\(p\\) curve) using those three sets of coefficients. Qualitatively describe the shapes of the curves, where the coefficients fall in likelihood landscape, and how well the curves match the data."
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html",
    "href": "course-materials/office-hours/week-6-oh.html",
    "title": "EDS222 week 6 office hours",
    "section": "",
    "text": "What’s the relationship between logit and likelihood?\nWhat is the logit?\nis Bernoulli(p) the same as logit(p)?"
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html#muddy-points",
    "href": "course-materials/office-hours/week-6-oh.html#muddy-points",
    "title": "EDS222 week 6 office hours",
    "section": "",
    "text": "What’s the relationship between logit and likelihood?\nWhat is the logit?\nis Bernoulli(p) the same as logit(p)?"
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html#what-is-the-logit",
    "href": "course-materials/office-hours/week-6-oh.html#what-is-the-logit",
    "title": "EDS222 week 6 office hours",
    "section": "What is the logit?",
    "text": "What is the logit?\nIt’s a transformation!\nProblem:\nGiven the equation\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\nDepending on the values on the right hand side of the equation, y can be any real number.\nIn logistic regression, we’ll need to calculate the likelihood of some 0’s and 1’s given a probability \\(p\\). If our \\(p\\) is outside [0, 1] that’s undefined.\nI can’t do this:\n\ndbinom(c(1, 1, 0), size = 1, prob = -1)\n\nWarning in dbinom(c(1, 1, 0), size = 1, prob = -1): NaNs produced\n\n\n[1] NaN NaN NaN\n\n\nWhen we try to use probabilities outside [0,1] we get Not A Number.\nThat’s where the logit comes in.\n\\[\nodds = \\frac{p}{1-p} \\\\\nlogit = log(odds)\n\\]\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\np &lt;- seq(0.01, 0.99, length.out = 100)\nodds &lt;- p / (1 - p)\nlogit &lt;- log(odds)\n\ntibble(p, odds, logit) %&gt;% \n  ggplot(aes(p, odds)) +\n  geom_line()\n\n\n\n\n\n\n\n# When p = 0.5, odds = 1\n# ditto,        logit = 0\n\ntibble(p, odds, logit) %&gt;% \n  ggplot(aes(p, logit)) +\n  geom_line()\n\n\n\n\n\n\n\n\nlogit converts what we have (0, 1) to what we can model (-Inf, Inf)\nAllows us to treat a probability as a linear model."
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html#whats-the-relationship-between-logit-and-likelihood",
    "href": "course-materials/office-hours/week-6-oh.html#whats-the-relationship-between-logit-and-likelihood",
    "title": "EDS222 week 6 office hours",
    "section": "What’s the relationship between logit and likelihood?",
    "text": "What’s the relationship between logit and likelihood?\n\\[\n\\text{endangered} \\sim Bernoulli(p) \\\\\nlogit(p) = \\beta_0 + \\beta_1 \\text{dist2coast}\n\\]\n\nSimulation\n\nset.seed(1422)\n# Step 1, generate predictors\ndist2coast &lt;- runif(100, 0, 100)\n# Step 2, choose our coefficients\nbeta_0 &lt;- 0\nbeta_1 &lt;- -0.1\n# Step 3, calculate random variable parameters (in this case, just p)\nlogit_p &lt;- beta_0 + beta_1 * dist2coast\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\np &lt;- inv_logit(logit_p)\n# Step 4, generate random numbers from the random variable\nendangered &lt;- rbinom(100, size = 1, prob = p)\n# Done! Simulated!\n\n# Let's visualize\ntibble(dist2coast, p, endangered) %&gt;% \n  ggplot(aes(dist2coast)) +\n  geom_point(aes(y = endangered)) +\n  geom_line(aes(y = p), color = \"firebrick\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\np declines as we move away from the coast (red line)\nonly “success” we saw was when p was closest to 1 (i.e. all the way to the left)\n\nNow, how would I make the red line increase as we move away from the coast?\n\nset.seed(1422)\n# Step 1, generate predictors\ndist2coast &lt;- runif(100, 0, 100)\n# Step 2, choose our coefficients\nbeta_0 &lt;- -1\nbeta_1 &lt;- 0.1\n# Step 3, calculate random variable parameters (in this case, just p)\nlogit_p &lt;- beta_0 + beta_1 * dist2coast\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\np &lt;- inv_logit(logit_p)\n# Step 4, generate random numbers from the random variable\nendangered &lt;- rbinom(100, size = 1, prob = p)\n# Done! Simulated!\n\n# Let's visualize\ntibble(dist2coast, p, endangered) %&gt;% \n  ggplot(aes(dist2coast)) +\n  geom_point(aes(y = endangered)) +\n  geom_line(aes(y = p), color = \"firebrick\") +\n  theme_bw()"
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html#is-bernoullip-the-same-as-logitp",
    "href": "course-materials/office-hours/week-6-oh.html#is-bernoullip-the-same-as-logitp",
    "title": "EDS222 week 6 office hours",
    "section": "Is Bernoulli(p) the same as logit(p)?",
    "text": "Is Bernoulli(p) the same as logit(p)?\nlogit is a transformation. It’s a formula applied to an input.\n\\[\nlogit(x) = log(\\frac{x}{1-x})\n\\]\nBernoulli is a random variable. “distributed as”\n\np &lt;- 0.75\nx &lt;- 0:1\nPMF_p &lt;- c(1 - p, p)\n\ntibble(p, PMF_p, x) %&gt;% \n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = PMF_p)) +\n  geom_point(aes(x, PMF_p), size = 2) +\n  expand_limits(y = 1) +\n  theme_bw()"
  },
  {
    "objectID": "course-materials/office-hours/week-6-oh.html#likelihood-walkthrough",
    "href": "course-materials/office-hours/week-6-oh.html#likelihood-walkthrough",
    "title": "EDS222 week 6 office hours",
    "section": "Likelihood walkthrough",
    "text": "Likelihood walkthrough\nWhat’s the likelihood of p being 0.5 if we observe {0, 1, 0, 0, 0, 1, 1}?\n\\[\nL=\\prod_i PMF(p_i) \\times (x==1)\n\\]\n\np_good &lt;- 0.5\nprod(dbinom(c(0, 1, 0, 0, 0, 1, 1), size = 1, prob = p_good))\n\n[1] 0.0078125\n\np_bad &lt;- 0.9\nprod(dbinom(c(0, 1, 0, 0, 0, 1, 1), size = 1, prob = p_bad))\n\n[1] 7.29e-05\n\n\nThe likelihood of p being 0.5 if we get 3 0’s and 3 1’s is much greater than the likelihood of p being 0.9 given the same observations.\nIf I got one 1 and three 0’s, what would be the most likely p?\n\np_good &lt;- 0.25\nprod(dbinom(c(1, 0, 0, 0), size = 1, prob = p_good))\n\n[1] 0.1054688\n\np_bad &lt;- 0.5\nprod(dbinom(c(1, 0, 0, 0), size = 1, prob = p_bad))\n\n[1] 0.0625\n\n\nWe care that 0.105 is greater than 0.0625. We don’t care about 0.105 in of itself.\nLet’s build that out to regression.\n\nset.seed(1422)\n# Step 1, generate predictors\ndist2coast &lt;- runif(100, 0, 100)\n# Step 2, choose our coefficients\nbeta_0 &lt;- -1\nbeta_1 &lt;- 0.1\n# Step 3, calculate random variable parameters (in this case, just p)\nlogit_p &lt;- beta_0 + beta_1 * dist2coast\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\np &lt;- inv_logit(logit_p)\n# Step 4, generate random numbers from the random variable\nendangered &lt;- rbinom(100, size = 1, prob = p)\n# Done! Simulated!\n\ntibble(dist2coast, p, endangered) %&gt;% \n  mutate(likelihood = dbinom(endangered, size = 1, prob = p))\n\n# A tibble: 100 × 4\n   dist2coast     p endangered likelihood\n        &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1      40.8  0.956          1      0.956\n 2      32.8  0.907          1      0.907\n 3      31.4  0.895          1      0.895\n 4      20.3  0.736          1      0.736\n 5      69.2  0.997          1      0.997\n 6      68.6  0.997          1      0.997\n 7      97.4  1.00           1      1.00 \n 8      53.1  0.987          1      0.987\n 9       4.47 0.365          0      0.635\n10      76.7  0.999          1      0.999\n# ℹ 90 more rows\n\n\n\nc(beta_0, beta_1)\n\n[1] -1.0  0.1\n\nlikelihood_fun &lt;- function(coef, data) {\n  # given x and coefs, figure out p\n  logit_p &lt;- coef[\"beta0\"] + coef[\"beta1\"] * data$dist2coast\n  p &lt;- inv_logit(logit_p)\n  # calculate the log likelihood of the observed, given p\n  loglik &lt;- dbinom(data$endangered, size = 1, prob = p, log = TRUE)\n  # Sum it up and flip the sign\n  -sum(loglik)\n}\n\nmpas &lt;- tibble(dist2coast, endangered)\nlikelihood_fun(c(beta0 = -1, beta1 = 0.1), mpas)\n\n[1] 11.4208\n\nlikelihood_fun(c(beta0 = -5, beta1 = 1), mpas)\n\n[1] 22.2768\n\n\nThe likelihood does not mean anything in of itself. It’s only important relatively."
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html",
    "href": "course-materials/labs/week-6-lab-key.html",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "",
    "text": "Is exposure to environmental hazards influenced by class or race and ethnicity? Switzer and Teodoro (Switzer and Teodoro 2017) argued the either-or framing of that question is misguided, and actually the interaction between class and race/ethnicity is the important question for environmental justice. Today’s lab will use logistic regression and interaction terms to investigate drinking water quality in the context of socio-economic status (SES), race, and ethnicity."
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#water-utility-and-violations-data",
    "href": "course-materials/labs/week-6-lab-key.html#water-utility-and-violations-data",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Water utility and violations data",
    "text": "Water utility and violations data\n\nSource: Safe Drinking Water Information System (SDWIS)\nTime period: 2010-2013 (4 years)\nSample size: 12,972 utilities\nCriteria: Local government-owned utilities serving populations of 10,000 or more"
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#demographic-data",
    "href": "course-materials/labs/week-6-lab-key.html#demographic-data",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Demographic data",
    "text": "Demographic data\n\nSource: US Census Bureau’s American Community Surveys (2010-2013)\nVariables included:\n\nPercent Hispanic population\nPercent Black population\nPercent of population with high school education\nPercent of population with bachelor’s degree\nPercent of population below poverty line\nMedian household income\n\n\nLoad the data and begin exploring.\n\nsuppressMessages(library(tidyverse))\ntheme_set(theme_bw())\n\ndrinking_water &lt;- read_csv(\"drinking_water.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#questions",
    "href": "course-materials/labs/week-6-lab-key.html#questions",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nWhat do you think each row represents?\nThe water utility and demographic data for a single district in a single year.\nWhat columns do you think represent:\n\nDrinking water health violations?\nhealth\nPercent Black and Hispanic population in the utility district?\npctblack and pcthisp, respectively\nMedian household income in the utility district?\nmedianincomehouse\n\nOne column includes a count of drinking water health violations. How would you create a new column with a binary variable representing whether there were any violations?\n\n\ndrinking_water$violations &lt;- ifelse(drinking_water$health &gt; 0, 1, 0)\n\n\nCreate a scatter plot of violations against race (percent Black population), ethnicity (percent Hispanic population), and SES (median household income). What visualization issues do we get with a scatter plot? How could you address that?\nPoints are overplotted, we can’t see the overall trend. One option: bin the data and calculate the mean.\n\n\nviolations_by_pctblack &lt;- drinking_water %&gt;% \n  mutate(pctblack = round(pctblack / 5) * 5) %&gt;% \n  group_by(pctblack) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(pctblack, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pctblack, color = \"red\")\n\nWarning: Removed 2351 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nviolations_by_pcthisp &lt;- drinking_water %&gt;% \n  mutate(pcthisp = round(pcthisp / 5) * 5) %&gt;% \n  group_by(pcthisp) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(pcthisp, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pcthisp, color = \"red\")\n\nWarning: Removed 2351 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nviolations_by_medianincomehouse &lt;- drinking_water %&gt;% \n  mutate(medianincomehouse = round(medianincomehouse / 1e4) * 1e4) %&gt;% \n  group_by(medianincomehouse) %&gt;% \n  summarize(violations = mean(violations))\n  \nggplot(drinking_water, aes(medianincomehouse, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_medianincomehouse, color = \"red\")\n\nWarning: Removed 2368 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#questions-1",
    "href": "course-materials/labs/week-6-lab-key.html#questions-1",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nPlot the residuals. What pattern do you notice?\nAll the residuals fall along two parallel lines - not normal!\n\n\ndrinking_water %&gt;% \n  select(violations, pcthisp) %&gt;% \n  drop_na() %&gt;% \n  mutate(resid = resid(pcthisp_lm)) %&gt;% \n  ggplot(aes(pcthisp, resid)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nPlot the raw data and the predicted values for violations. Are there any obvious problems?\nThe predicted values fall between 0 and 1, so they could conceivably be interpreted as probabilities. But the raw data are far from the line.\n\n\nggplot(drinking_water, aes(pcthisp, violations)) +\n  geom_point() +\n  geom_point(data = violations_by_pcthisp, color = \"red\") +\n  geom_abline(intercept = coef(pcthisp_lm)[1],\n              slope = coef(pcthisp_lm)[2])\n\nWarning: Removed 2351 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#negative-log-likelihood",
    "href": "course-materials/labs/week-6-lab-key.html#negative-log-likelihood",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Negative log likelihood",
    "text": "Negative log likelihood\nEstimting coefficients is an optimization problem: what combination yields the maximum likelihood? There are two things we can do to make our problem more tractable for optimization.\n\nRecall that the likelihood function involves a product. Multiplication is computationally costly (compared to addition) and multiplying small numbers is very error prone. We can avoid this problem by working with logarithms. The log of a product is the sum of the logs: \\(log(a \\times b) = log(a) + log(b)\\). Logarithms are monotonically increasing, which means if \\(a &gt; b\\) then \\(log(a) &gt; log(b)\\). This useful property means we can maximize the sum of the log likelihoods (which is quick and robust to errors) instead of the product of likelihoods (which is slow and error prone).\nOptimization algorithms typically find the minimum value. They’re intended to look for valleys, not peaks. So instead of maximizing the log likelihood, we can minimize the negative log likelihood.\n\nThat seems confusing! Let’s write the code and so we can see what’s happening. Do the following:\n\nWrite a likelihood function for the violations and percent Hispanic model. This should calculate the negative log likelihood of a set of coefficients, conditional on the data.\nCall an optimization function to find the maximum likelihood parameters.\n\nIt will help to keep the model formulation handy:\n\\[\n\\begin{align}\n\\text{violations} &\\sim Bernoulli(p) \\\\\nlogit(p) &= \\beta_0 + \\beta_1 \\text{percentHispanic}\n\\end{align}\n\\]\n\n# Inverse logit utility function\ninv_logit &lt;- function(x) exp(x) / (1 + exp(x))\n  \n# Likelihood of the coefficients, given the data\nlikelihood_fun &lt;- function(coefficients, data) {\n  # Calculate logit(p) based on coefficients and predictor\n  logit_p &lt;- coefficients[\"beta0\"] + coefficients[\"beta1\"] * data$pcthisp\n  # Invert the logit to get p\n  p &lt;- inv_logit(logit_p)\n  # Use the PMF of the Bernoulli to get our log likelihoods\n  loglik &lt;- dbinom(data$violations, size = 1, prob = p, log = TRUE)\n  # Sum the negative log likelihood\n  sum(-loglik)\n}\n\n# Use an optimization function to get the maximum likelihood coefficients\ndrinking_water_complete &lt;- drop_na(drinking_water, pcthisp, violations)\ncoef_optim &lt;- optim(c(beta0 = 0, beta1 = 0), \n                    likelihood_fun, \n                    data = drinking_water_complete)"
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#questions-2",
    "href": "course-materials/labs/week-6-lab-key.html#questions-2",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nWhat were your maximum likelihood estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)?\n\n\ncoef_optim$par\n\n       beta0        beta1 \n-2.422124762  0.004398142 \n\n\n\nWhat’s the predicted probability of drinking water violations for communities with 0%, 50%, and 100% Hispanic population?\nPlot the predicted probability across the whole range 0-100% Hispanic.\n\n\nbeta0_hat &lt;- coef_optim$par[\"beta0\"]\nbeta1_hat &lt;- coef_optim$par[\"beta1\"]\npcthisp &lt;- seq(0, 100, by = 1)\nlogit_p &lt;- beta0_hat + beta1_hat * pcthisp\np &lt;- inv_logit(logit_p)\n\np[pcthisp %in% c(0, 50, 100)]\n\n[1] 0.08150106 0.09955152 0.12107275\n\ntibble(pcthisp, p) %&gt;% \n  ggplot(aes(pcthisp, p)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nHow much does the probability of a drinking water violation change when percent Hispanic population increases from 10 to 20%, 45 to 55%, and 80 to 90%?\n\n\np[pcthisp %in% c(20, 55, 90)] - p[pcthisp %in% c(10, 45, 80)]\n\n[1] 0.003478296 0.003942686 0.004450113\n\n\n\nHow would you interpret the coefficients? What do the slope and intercept mean in this context? Where is the relationship linear, and where is it non-linear?\nThe coefficients define a linear function in logit space. The intercept is the value of the logit(p) when percent Hispanic is 0. The slope is the change in logit(p) for a 1-percent increase in Hispanic population. p itself is non-linearly related to percent Hispanic population, because the logit function is not linear.\nCreate a “DEM” of the likelihood landscape for \\(\\beta_0\\) and \\(\\beta_1\\). Choose a range of \\(\\beta_0\\) and \\(\\beta_1\\) values around your best estimates, calculate the likelihood for each combination, and create a figure with \\(\\beta_0\\) on the x-axis, \\(\\beta_1\\) on the y-axis, and the likelihood as the fill. Add a point for \\((\\hat\\beta_0, \\hat\\beta_1)\\).\nBonus problem: add contours!\n\n\nlikelihood_dem &lt;- expand_grid(\n  beta0 = seq(-3.5, -1.5, length.out = 1e2),\n  beta1 = seq(0.001, 0.008, length.out = 1e2)\n) %&gt;% \n  mutate(coefficients = mapply(function(b0, b1) c(beta0 = b0, beta1 = b1),\n                               beta0, beta1,\n                               SIMPLIFY = FALSE),\n         negloglik = sapply(coefficients, \n                            likelihood_fun, \n                            data = drinking_water_complete),\n         likelihood = exp(-negloglik))\n\nggplot(likelihood_dem, aes(beta0, beta1, fill = negloglik)) +\n  geom_raster() +\n  geom_point(x = beta0_hat, y = beta1_hat, color = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week-6-lab-key.html#questions-3",
    "href": "course-materials/labs/week-6-lab-key.html#questions-3",
    "title": "EDS222 Week 6 Lab: Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nHow would you fit a model that includes an interaction term between ethnicity and SES?\n\n\ninteraction_glm &lt;- glm(violations ~ pcthisp * medianincomehouse, \n                       family = binomial(link = \"logit\"),\n                       data = drinking_water)\nsummary(interaction_glm)\n\n\nCall:\nglm(formula = violations ~ pcthisp * medianincomehouse, family = binomial(link = \"logit\"), \n    data = drinking_water)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -2.017e+00  5.174e-02 -38.984  &lt; 2e-16 ***\npcthisp                    2.463e-02  2.696e-03   9.136  &lt; 2e-16 ***\nmedianincomehouse         -8.253e-06  1.169e-06  -7.063 1.63e-12 ***\npcthisp:medianincomehouse -5.497e-07  7.098e-08  -7.745 9.55e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30506  on 52162  degrees of freedom\nResidual deviance: 30222  on 52159  degrees of freedom\n  (2368 observations deleted due to missingness)\nAIC: 30230\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nCreate a figure similar to Fig. 1 in Switzer and Teodoro (2017). Put percent Hispanic population on the x-axis, median household income on the y-axis, and make the fill the probability of a water quality violation.\n\n\npredictions &lt;- expand_grid(\n  pcthisp = seq(0, 100, length.out = 100),\n  medianincomehouse = seq(2500, 250000, length.out = 100)\n) %&gt;% \n  mutate(violations = predict(interaction_glm, \n                              newdata = ., \n                              type = \"response\"))\n\nggplot(predictions, aes(pcthisp, \n                        medianincomehouse, \n                        fill = violations)) +\n  geom_raster() +\n  scale_fill_gradient(low = \"navy\", high = \"firebrick\") +\n  labs(x = \"Percent Hispanic population\",\n       y = \"Median household income\",\n       fill = \"Pr(violation)\")\n\n\n\n\n\n\n\n\n\nInterpret the predicted surface. How does SES influence the relationship between ethnicity and exposure to environmental hazards? What is the “slope” of the probability of a violation w.r.t. percent Hispanic population at low, medium, and high median household income levels?\nAt the low end of the SES axis, Pr(violation) increases steeply with percent Hispanic population, but the relationship is flatter at higher income levels. At a low median household income level ($30k), the “slope” is 0.008. At moderate ($45k) and high ($60k) median household income levels, the “slope” is ~0 and -0.008, respectively. Note: there are very few data points in the upper-right corner of the figure, so the negative slope observed for high median household income level may be an extrapolation issue."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html",
    "href": "course-materials/labs/week-8-lab-key.html",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Lead is an important contaminant in urban areas with well-known impacts on human health, but do non-human animals also face risks from lead exposure? Hitt et al. investigated this question by measuring lead levels in soil and the blood of breeding mockingbirds, as well as egg hatching and offspring development (Hitt et al. 2023). We will replicate parts of their analysis here.\n\n\n\nThe data for this study were deposited in the Dryad Data Repository and are available here. Download the full dataset and put them in the appropriate folder of your RStudio project.\nRead the northern mockingbird nestling data (“NOMO_Nest_Data.csv”) and nestling lead data (“NestlingPb.csv”) into data frames called nest_data and nestlingpb_data, respectively. Filter both data frames to the Uptown and Lakeshore neighborhoods.\n\nnest_data &lt;- read_csv(\"data/NOMO_Nest_Data.csv\") %&gt;% \n  filter(hood %in% c(\"up\", \"ls\"))\n\nRows: 214 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): year, nest, hood, status, lay.month\ndbl (6): nest_id, bin.status, clutch, pred.eggs, survive, product\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnestlingpb_data &lt;- read_csv(\"data/NestlingPb.csv\") %&gt;% \n  filter(hood %in% c(\"uptown\", \"lakeshore\"))\n\nRows: 92 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): age, hood, hood_pb, bloodtube, notes\ndbl (11): nomo, nest_id, ug_g_drywt, blood_ng_ml_pbwt, blood_ug_dl_pbwt, blo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nDo mockingbirds in a neighborhood with higher lead concentration have less successful nests?\nWe’ll investigate this question using randomization hypothesis testing.\n\n\nnest_data contains columns hood and bin.status, representing the neighborhood and binary status (at least one chick fledged or not) for each monitored nest. Visualize how nest success (i.e., binary status) varied by neighborhood.\n\nggplot(nest_data, aes(hood, fill = factor(bin.status))) +\n  geom_bar() +\n  scale_fill_brewer(\"Nest success\", palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nH0: Neighborhood does not have an effect on nest success\nHA: Neighborhood has an effect on nest success\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in proportions\n2) How would you calculate it?\n\nnest_success &lt;- nest_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(prop = sum(bin.status) / n())\n\npoint_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n\npoint_estimate_success\n\n[1] -0.05050505\n\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  nest_success &lt;- nest_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(prop = sum(bin.status) / n())\n  \n  point_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n  \n  point_estimate_success\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, color = \"cornflowerblue\", fill = NA) +\n  geom_vline(xintercept = point_estimate_success, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_success)) / length(null_dist)\n\n[1] 0.497\n\n\n\n\n\nWe’re using a threshold of \\(\\alpha=0.05\\). A p-value of 0.466 is greater than our threshold, so we fail to reject the null.\nThis DOES NOT mean we accept the null. We DO NOT claim neighborhood does not have an effect on nest success. Rather, we say we could not detect an effect, if one exists.\n\n\n\n\nNow it’s your turn. Perform a similar analysis to investigate whether nestling blood Pb levels vary by neighborhood. Use nestlingpb_data for this part, column blood_ug_dl_pbwt.\nFirst, visualize the blood Pb levels by neighborhood. What’s an appropriate type of visualization for this?\n\nggplot(nestlingpb_data, aes(hood, blood_ug_dl_pbwt)) +\n  geom_boxplot() +\n  labs(y = \"Blood Pb (ug dl^-1)\")\n\n\n\n\n\n\n\n\n\n\nH0: Neighborhood does not have an effect on nestling Pb levels\nHA: Neighborhood has an effect on nestling Pb levels\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in means\n2) How would you calculate it?\n\nbloodpb &lt;- nestlingpb_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(mean = mean(blood_ug_dl_pbwt))\n\npoint_estimate_bloodpb &lt;- bloodpb$mean[2] - bloodpb$mean[1]\n\npoint_estimate_bloodpb\n\n[1] 10.74248\n\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  bloodpb &lt;- nestlingpb_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(mean = mean(blood_ug_dl_pbwt))\n  \n  point_estimate_bloodpb &lt;- bloodpb$mean[2] - bloodpb$mean[1]\n  \n  point_estimate_bloodpb\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, color = \"cornflowerblue\", fill = NA) +\n  geom_vline(xintercept = point_estimate_bloodpb, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_bloodpb)) / length(null_dist)\n\n[1] 0\n\n\n\n\n\nWe’re using a threshold of \\(\\alpha=0.05\\). A p-value of 0 is less than our threshold, so we reject the null. We find neighborhood does have an effect on nestling blood Pb levels.\n\n\n\n\n\nRather than using randomization to simulate the null distribution, it’s often easier to approximate it as a normal distribution.\nDoes nestling blood Pb level have a relationship with feather Pb level?\nFirst, visualize the relationship between the two variables.\n\nggplot(nestlingpb_data, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 45 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 45 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nH0: Blood Pb level does not have an effect on feather Pb level\nHA: Blood Pb level has an effect on feather Pb level\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nRegression coefficient\n2) How would you calculate it?\n\nblood_feathers_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt,\n                        nestlingpb_data)\nsummary(blood_feathers_lm)\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = nestlingpb_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.117  -2.072   1.186   3.175  15.438 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.9548     1.6799  -2.354   0.0289 *  \nblood_ug_dl_pbwt   0.6886     0.0934   7.372 4.02e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.354 on 20 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.731, Adjusted R-squared:  0.7176 \nF-statistic: 54.35 on 1 and 20 DF,  p-value: 4.023e-07\n\n\n\n\n\nUse the standard error of the regression coefficient to visualize the distribution of the sample statistic under the null hypothesis.\n\nbeta1_estimate &lt;- summary(blood_feathers_lm)$coefficients[2, 1]\nbeta1_se &lt;- summary(blood_feathers_lm)$coefficients[2, 2]\ntibble(beta1 = seq(-(beta1_estimate + beta1_se),\n                   beta1_estimate + beta1_se,\n                   length.out = 200),\n       density = dnorm(beta1, mean = 0, sd = beta1_se)) %&gt;% \n  ggplot(aes(beta1, density)) +\n  geom_line(color = \"cornflowerblue\") +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value? Use pnorm() to get the probability of a point estimate at least as extreme as the observed.\n\npval &lt;- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)\npval\n\n[1] 1.677745e-13\n\n\n\n\n\n\n\n\nNote\n\n\n\nOur calculate p-value is much lower than the p-value from the summary of our linear model. In class I told you lm() uses a Student’s t-distribution instead of a normal distribution for calculating the p-value. When sample sizes are large, the normal distribution and t-distribution are virtually identical. With only 22 complete data points, our sample size is relatively small. Therefore the t-distribution has thicker tails and yields a larger p-value. Hence the discrepancy.\n\n\n\n\n\nOur p-value is much less than 0.05, so we reject the null and interpret our model to say blood Pb levels have an effect on feather Pb levels.\n\n\n\nRevisit the visualization of the blood and feather Pb levels. One point seems to be an extreme outlier, and it seems to be exerting a strong influence on our model. Repeat the previous analysis with that point removed, then answer the question below.\n\nno_outlier &lt;- nestlingpb_data %&gt;% \n  filter(blood_ug_dl_pbwt &lt; 40)\n\nggplot(no_outlier, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 44 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 44 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nblood_feathers_lm2 &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt,\n                         no_outlier)\nsummary(blood_feathers_lm2)\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = no_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6687 -0.2521 -0.1980 -0.1419  4.2933 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       0.12645    0.34985   0.361  0.72175   \nblood_ug_dl_pbwt  0.08843    0.03013   2.934  0.00851 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.158 on 19 degrees of freedom\n  (44 observations deleted due to missingness)\nMultiple R-squared:  0.3119,    Adjusted R-squared:  0.2756 \nF-statistic: 8.611 on 1 and 19 DF,  p-value: 0.008509\n\nbeta1_estimate &lt;- summary(blood_feathers_lm2)$coefficients[2, 1]\nbeta1_se &lt;- summary(blood_feathers_lm2)$coefficients[2, 2]\ntibble(beta1 = seq(-(beta1_estimate + beta1_se),\n                   beta1_estimate + beta1_se,\n                   length.out = 200),\n       density = dnorm(beta1, mean = 0, sd = beta1_se)) %&gt;% \n  ggplot(aes(beta1, density)) +\n  geom_line(color = \"cornflowerblue\") +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")\n\n\n\n\n\n\n\npval &lt;- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)\npval\n\n[1] 0.003341459\n\n\nQuestion: Of the two analyses (with and without the outlier), which had a lower p-value? Does that make it a better analysis?\nThere is no objectively correct answer to this question. Removing the outlier increased our p-value. If the outlier is not representative of our system for some reason (e.g., it’s the result of a lab error), then removing it and getting a higher p-value is a more appropriate way to interpret the data. However, the outlier may represent a real potential outcome in our system. If that’s the case, we should keep it and perhaps consider an alternative to OLS that’s less sensitive to the allure of outliers.\n\n\n\n\nFor the last part of today’s lab we will construct confidence intervals around the point estimate of the blood Pb level coefficient. Here’s the plan:\n\nSimulate a population of nestlings, with the same relationship between blood and feather Pb levels as the observed sample.\nDraw a new sample of nestlings from the simulated population. Create a confidence interval for the point estimate of the blood Pb level coefficient in this sample.\nRepeat the process 100 times.\n\n~95% of our 95% CIs should contain the population parameter.\n\n\n\n# Simulate the population\n# Extract our estimates for beta0, beta1, and sigma (i.e., the SD of the data\n# around the mean response)\nbeta0_estimate &lt;- summary(blood_feathers_lm2)$coefficients[1,1]\nbeta1_estimate &lt;- summary(blood_feathers_lm2)$coefficients[2,1]\nsigma &lt;- summary(blood_feathers_lm2)$sigma\nblood_feather_pop &lt;- tibble(\n  # Predictor (uniformly distributed)\n  blood_ug_dl_pbwt = runif(1e4, \n                           min(no_outlier$blood_ug_dl_pbwt),\n                           max(no_outlier$blood_ug_dl_pbwt)),\n  # Mean response\n  mean_feather_ug_g_pbwt = beta0_estimate + beta1_estimate * blood_ug_dl_pbwt,\n  # Simulated response (mean and standard deviation accounted for)\n  feather_ug_g_pbwt = rnorm(1e4, mean = mean_feather_ug_g_pbwt, sd = sigma)\n)\n\n# Visualize\nggplot(blood_feather_pop, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point(shape = 21)\n\n\n\n\n\n\n\n# Verify the simulated population's coefficients match the inputs\nc(beta0_estimate, beta1_estimate)\n\n[1] 0.12645385 0.08842879\n\nsummary(lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_pop))\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = blood_feather_pop)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0978 -0.7767 -0.0063  0.7730  4.3946 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.162835   0.023506   6.927 4.55e-12 ***\nblood_ug_dl_pbwt 0.087599   0.001032  84.843  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.158 on 9998 degrees of freedom\nMultiple R-squared:  0.4186,    Adjusted R-squared:  0.4185 \nF-statistic:  7198 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Draw a sample\nblood_feather_sample &lt;- sample_n(blood_feather_pop, 21)\n# Calculate the point estimate and standard error\nsample_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_sample)\npe &lt;- summary(sample_lm)$coefficients[2, 1]\nse &lt;- summary(sample_lm)$coefficients[2, 2]\n# Construct the confidence interval\nsample_ci &lt;- c(point_estimate = pe,\n               ci95_lower = pe - 1.96 * se,\n               ci95_upper = pe + 1.96 * se)\nsample_ci\n\npoint_estimate     ci95_lower     ci95_upper \n    0.08666608     0.04214311     0.13118904 \n\n\n\n\n\nHow many 95% CIs contain the population parameter?\n\nrepeat_ci &lt;- replicate(100, {\n  blood_feather_sample &lt;- sample_n(blood_feather_pop, 21)\n  sample_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_sample)\n  pe &lt;- summary(sample_lm)$coefficients[2, 1]\n  se &lt;- summary(sample_lm)$coefficients[2, 2]\n  sample_ci &lt;- c(point_estimate = pe,\n                 ci95_lower = pe - 1.96 * se,\n                 ci95_upper = pe + 1.96 * se)\n  sample_ci\n})\n\ntibble(point_estimate = repeat_ci[1, ],\n       ci95_lower = repeat_ci[2, ],\n       ci95_upper = repeat_ci[3, ],\n       ci = 1:100) %&gt;% \n  mutate(valid = beta1_estimate &gt;= ci95_lower & \n           beta1_estimate &lt;= ci95_upper) %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = point_estimate, \n                      xmin = ci95_lower,\n                      xmax = ci95_upper,\n                      y = ci,\n                      color = valid)) +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomization allows us to simulate the null distribution, which we can use to quantify the probability of our result if the null hypothesis is true.\n\nsample() and replicate() are helpful here!\nRandomization doesn’t make assumptions about the normality of the sample statistic, but it does assume the sample is representative of the population.\n\nBy assuming the sample statistic is normally distributed, we can use standard errors to conduct hypothesis testing.\n\nR will calculate standard errors for us for most sample statistics, such as regression coefficients.\n\nWe can also use standard errors to construct confidence intervals.\n\nBy simulating a population, we demonstrated ~95% of 95% CIs will contain the population parameter."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#background",
    "href": "course-materials/labs/week-8-lab-key.html#background",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Lead is an important contaminant in urban areas with well-known impacts on human health, but do non-human animals also face risks from lead exposure? Hitt et al. investigated this question by measuring lead levels in soil and the blood of breeding mockingbirds, as well as egg hatching and offspring development (Hitt et al. 2023). We will replicate parts of their analysis here."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#get-the-data",
    "href": "course-materials/labs/week-8-lab-key.html#get-the-data",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "The data for this study were deposited in the Dryad Data Repository and are available here. Download the full dataset and put them in the appropriate folder of your RStudio project.\nRead the northern mockingbird nestling data (“NOMO_Nest_Data.csv”) and nestling lead data (“NestlingPb.csv”) into data frames called nest_data and nestlingpb_data, respectively. Filter both data frames to the Uptown and Lakeshore neighborhoods.\n\nnest_data &lt;- read_csv(\"data/NOMO_Nest_Data.csv\") %&gt;% \n  filter(hood %in% c(\"up\", \"ls\"))\n\nRows: 214 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): year, nest, hood, status, lay.month\ndbl (6): nest_id, bin.status, clutch, pred.eggs, survive, product\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnestlingpb_data &lt;- read_csv(\"data/NestlingPb.csv\") %&gt;% \n  filter(hood %in% c(\"uptown\", \"lakeshore\"))\n\nRows: 92 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): age, hood, hood_pb, bloodtube, notes\ndbl (11): nomo, nest_id, ug_g_drywt, blood_ng_ml_pbwt, blood_ug_dl_pbwt, blo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#hypothesis-testing-by-randomization",
    "href": "course-materials/labs/week-8-lab-key.html#hypothesis-testing-by-randomization",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Do mockingbirds in a neighborhood with higher lead concentration have less successful nests?\nWe’ll investigate this question using randomization hypothesis testing.\n\n\nnest_data contains columns hood and bin.status, representing the neighborhood and binary status (at least one chick fledged or not) for each monitored nest. Visualize how nest success (i.e., binary status) varied by neighborhood.\n\nggplot(nest_data, aes(hood, fill = factor(bin.status))) +\n  geom_bar() +\n  scale_fill_brewer(\"Nest success\", palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nH0: Neighborhood does not have an effect on nest success\nHA: Neighborhood has an effect on nest success\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in proportions\n2) How would you calculate it?\n\nnest_success &lt;- nest_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(prop = sum(bin.status) / n())\n\npoint_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n\npoint_estimate_success\n\n[1] -0.05050505\n\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  nest_success &lt;- nest_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(prop = sum(bin.status) / n())\n  \n  point_estimate_success &lt;- nest_success$prop[2] - nest_success$prop[1]\n  \n  point_estimate_success\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, color = \"cornflowerblue\", fill = NA) +\n  geom_vline(xintercept = point_estimate_success, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_success)) / length(null_dist)\n\n[1] 0.497\n\n\n\n\n\nWe’re using a threshold of \\(\\alpha=0.05\\). A p-value of 0.466 is greater than our threshold, so we fail to reject the null.\nThis DOES NOT mean we accept the null. We DO NOT claim neighborhood does not have an effect on nest success. Rather, we say we could not detect an effect, if one exists.\n\n\n\n\nNow it’s your turn. Perform a similar analysis to investigate whether nestling blood Pb levels vary by neighborhood. Use nestlingpb_data for this part, column blood_ug_dl_pbwt.\nFirst, visualize the blood Pb levels by neighborhood. What’s an appropriate type of visualization for this?\n\nggplot(nestlingpb_data, aes(hood, blood_ug_dl_pbwt)) +\n  geom_boxplot() +\n  labs(y = \"Blood Pb (ug dl^-1)\")\n\n\n\n\n\n\n\n\n\n\nH0: Neighborhood does not have an effect on nestling Pb levels\nHA: Neighborhood has an effect on nestling Pb levels\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nDifference in means\n2) How would you calculate it?\n\nbloodpb &lt;- nestlingpb_data %&gt;% \n  group_by(hood) %&gt;% \n  summarize(mean = mean(blood_ug_dl_pbwt))\n\npoint_estimate_bloodpb &lt;- bloodpb$mean[2] - bloodpb$mean[1]\n\npoint_estimate_bloodpb\n\n[1] 10.74248\n\n\n\n\n\nUse randomization to simulate the distribution of the sample statistic under the null hypothesis.\n\nnull_dist &lt;- replicate(1000, {\n  bloodpb &lt;- nestlingpb_data %&gt;% \n    mutate(hood = sample(hood, n())) %&gt;% \n    group_by(hood) %&gt;% \n    summarize(mean = mean(blood_ug_dl_pbwt))\n  \n  point_estimate_bloodpb &lt;- bloodpb$mean[2] - bloodpb$mean[1]\n  \n  point_estimate_bloodpb\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, color = \"cornflowerblue\", fill = NA) +\n  geom_vline(xintercept = point_estimate_bloodpb, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value?\n\nsum(abs(null_dist) &gt; abs(point_estimate_bloodpb)) / length(null_dist)\n\n[1] 0\n\n\n\n\n\nWe’re using a threshold of \\(\\alpha=0.05\\). A p-value of 0 is less than our threshold, so we reject the null. We find neighborhood does have an effect on nestling blood Pb levels."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#hypothesis-testing-by-normal-approximation",
    "href": "course-materials/labs/week-8-lab-key.html#hypothesis-testing-by-normal-approximation",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Rather than using randomization to simulate the null distribution, it’s often easier to approximate it as a normal distribution.\nDoes nestling blood Pb level have a relationship with feather Pb level?\nFirst, visualize the relationship between the two variables.\n\nggplot(nestlingpb_data, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 45 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 45 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nH0: Blood Pb level does not have an effect on feather Pb level\nHA: Blood Pb level has an effect on feather Pb level\n\n\n\n1) What is the relevant sample statistic for our hypothesis?\nRegression coefficient\n2) How would you calculate it?\n\nblood_feathers_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt,\n                        nestlingpb_data)\nsummary(blood_feathers_lm)\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = nestlingpb_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.117  -2.072   1.186   3.175  15.438 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.9548     1.6799  -2.354   0.0289 *  \nblood_ug_dl_pbwt   0.6886     0.0934   7.372 4.02e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.354 on 20 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.731, Adjusted R-squared:  0.7176 \nF-statistic: 54.35 on 1 and 20 DF,  p-value: 4.023e-07\n\n\n\n\n\nUse the standard error of the regression coefficient to visualize the distribution of the sample statistic under the null hypothesis.\n\nbeta1_estimate &lt;- summary(blood_feathers_lm)$coefficients[2, 1]\nbeta1_se &lt;- summary(blood_feathers_lm)$coefficients[2, 2]\ntibble(beta1 = seq(-(beta1_estimate + beta1_se),\n                   beta1_estimate + beta1_se,\n                   length.out = 200),\n       density = dnorm(beta1, mean = 0, sd = beta1_se)) %&gt;% \n  ggplot(aes(beta1, density)) +\n  geom_line(color = \"cornflowerblue\") +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the p-value? Use pnorm() to get the probability of a point estimate at least as extreme as the observed.\n\npval &lt;- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)\npval\n\n[1] 1.677745e-13\n\n\n\n\n\n\n\n\nNote\n\n\n\nOur calculate p-value is much lower than the p-value from the summary of our linear model. In class I told you lm() uses a Student’s t-distribution instead of a normal distribution for calculating the p-value. When sample sizes are large, the normal distribution and t-distribution are virtually identical. With only 22 complete data points, our sample size is relatively small. Therefore the t-distribution has thicker tails and yields a larger p-value. Hence the discrepancy.\n\n\n\n\n\nOur p-value is much less than 0.05, so we reject the null and interpret our model to say blood Pb levels have an effect on feather Pb levels.\n\n\n\nRevisit the visualization of the blood and feather Pb levels. One point seems to be an extreme outlier, and it seems to be exerting a strong influence on our model. Repeat the previous analysis with that point removed, then answer the question below.\n\nno_outlier &lt;- nestlingpb_data %&gt;% \n  filter(blood_ug_dl_pbwt &lt; 40)\n\nggplot(no_outlier, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 44 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 44 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nblood_feathers_lm2 &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt,\n                         no_outlier)\nsummary(blood_feathers_lm2)\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = no_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6687 -0.2521 -0.1980 -0.1419  4.2933 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       0.12645    0.34985   0.361  0.72175   \nblood_ug_dl_pbwt  0.08843    0.03013   2.934  0.00851 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.158 on 19 degrees of freedom\n  (44 observations deleted due to missingness)\nMultiple R-squared:  0.3119,    Adjusted R-squared:  0.2756 \nF-statistic: 8.611 on 1 and 19 DF,  p-value: 0.008509\n\nbeta1_estimate &lt;- summary(blood_feathers_lm2)$coefficients[2, 1]\nbeta1_se &lt;- summary(blood_feathers_lm2)$coefficients[2, 2]\ntibble(beta1 = seq(-(beta1_estimate + beta1_se),\n                   beta1_estimate + beta1_se,\n                   length.out = 200),\n       density = dnorm(beta1, mean = 0, sd = beta1_se)) %&gt;% \n  ggplot(aes(beta1, density)) +\n  geom_line(color = \"cornflowerblue\") +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")\n\n\n\n\n\n\n\npval &lt;- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)\npval\n\n[1] 0.003341459\n\n\nQuestion: Of the two analyses (with and without the outlier), which had a lower p-value? Does that make it a better analysis?\nThere is no objectively correct answer to this question. Removing the outlier increased our p-value. If the outlier is not representative of our system for some reason (e.g., it’s the result of a lab error), then removing it and getting a higher p-value is a more appropriate way to interpret the data. However, the outlier may represent a real potential outcome in our system. If that’s the case, we should keep it and perhaps consider an alternative to OLS that’s less sensitive to the allure of outliers."
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#confidence-intervals",
    "href": "course-materials/labs/week-8-lab-key.html#confidence-intervals",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "For the last part of today’s lab we will construct confidence intervals around the point estimate of the blood Pb level coefficient. Here’s the plan:\n\nSimulate a population of nestlings, with the same relationship between blood and feather Pb levels as the observed sample.\nDraw a new sample of nestlings from the simulated population. Create a confidence interval for the point estimate of the blood Pb level coefficient in this sample.\nRepeat the process 100 times.\n\n~95% of our 95% CIs should contain the population parameter.\n\n\n\n# Simulate the population\n# Extract our estimates for beta0, beta1, and sigma (i.e., the SD of the data\n# around the mean response)\nbeta0_estimate &lt;- summary(blood_feathers_lm2)$coefficients[1,1]\nbeta1_estimate &lt;- summary(blood_feathers_lm2)$coefficients[2,1]\nsigma &lt;- summary(blood_feathers_lm2)$sigma\nblood_feather_pop &lt;- tibble(\n  # Predictor (uniformly distributed)\n  blood_ug_dl_pbwt = runif(1e4, \n                           min(no_outlier$blood_ug_dl_pbwt),\n                           max(no_outlier$blood_ug_dl_pbwt)),\n  # Mean response\n  mean_feather_ug_g_pbwt = beta0_estimate + beta1_estimate * blood_ug_dl_pbwt,\n  # Simulated response (mean and standard deviation accounted for)\n  feather_ug_g_pbwt = rnorm(1e4, mean = mean_feather_ug_g_pbwt, sd = sigma)\n)\n\n# Visualize\nggplot(blood_feather_pop, aes(blood_ug_dl_pbwt, feather_ug_g_pbwt)) +\n  geom_point(shape = 21)\n\n\n\n\n\n\n\n# Verify the simulated population's coefficients match the inputs\nc(beta0_estimate, beta1_estimate)\n\n[1] 0.12645385 0.08842879\n\nsummary(lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_pop))\n\n\nCall:\nlm(formula = feather_ug_g_pbwt ~ blood_ug_dl_pbwt, data = blood_feather_pop)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0978 -0.7767 -0.0063  0.7730  4.3946 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.162835   0.023506   6.927 4.55e-12 ***\nblood_ug_dl_pbwt 0.087599   0.001032  84.843  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.158 on 9998 degrees of freedom\nMultiple R-squared:  0.4186,    Adjusted R-squared:  0.4185 \nF-statistic:  7198 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Draw a sample\nblood_feather_sample &lt;- sample_n(blood_feather_pop, 21)\n# Calculate the point estimate and standard error\nsample_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_sample)\npe &lt;- summary(sample_lm)$coefficients[2, 1]\nse &lt;- summary(sample_lm)$coefficients[2, 2]\n# Construct the confidence interval\nsample_ci &lt;- c(point_estimate = pe,\n               ci95_lower = pe - 1.96 * se,\n               ci95_upper = pe + 1.96 * se)\nsample_ci\n\npoint_estimate     ci95_lower     ci95_upper \n    0.08666608     0.04214311     0.13118904 \n\n\n\n\n\nHow many 95% CIs contain the population parameter?\n\nrepeat_ci &lt;- replicate(100, {\n  blood_feather_sample &lt;- sample_n(blood_feather_pop, 21)\n  sample_lm &lt;- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt, blood_feather_sample)\n  pe &lt;- summary(sample_lm)$coefficients[2, 1]\n  se &lt;- summary(sample_lm)$coefficients[2, 2]\n  sample_ci &lt;- c(point_estimate = pe,\n                 ci95_lower = pe - 1.96 * se,\n                 ci95_upper = pe + 1.96 * se)\n  sample_ci\n})\n\ntibble(point_estimate = repeat_ci[1, ],\n       ci95_lower = repeat_ci[2, ],\n       ci95_upper = repeat_ci[3, ],\n       ci = 1:100) %&gt;% \n  mutate(valid = beta1_estimate &gt;= ci95_lower & \n           beta1_estimate &lt;= ci95_upper) %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = point_estimate, \n                      xmin = ci95_lower,\n                      xmax = ci95_upper,\n                      y = ci,\n                      color = valid)) +\n  geom_vline(xintercept = beta1_estimate, color = \"firebrick\")"
  },
  {
    "objectID": "course-materials/labs/week-8-lab-key.html#recap",
    "href": "course-materials/labs/week-8-lab-key.html#recap",
    "title": "EDS222 Week 8 Lab: Hypothesis testing",
    "section": "",
    "text": "Randomization allows us to simulate the null distribution, which we can use to quantify the probability of our result if the null hypothesis is true.\n\nsample() and replicate() are helpful here!\nRandomization doesn’t make assumptions about the normality of the sample statistic, but it does assume the sample is representative of the population.\n\nBy assuming the sample statistic is normally distributed, we can use standard errors to conduct hypothesis testing.\n\nR will calculate standard errors for us for most sample statistics, such as regression coefficients.\n\nWe can also use standard errors to construct confidence intervals.\n\nBy simulating a population, we demonstrated ~95% of 95% CIs will contain the population parameter."
  }
]